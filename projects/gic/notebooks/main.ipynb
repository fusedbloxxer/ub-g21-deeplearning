{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries\n",
    "import os as so\n",
    "import sys as s\n",
    "import pathlib as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as om\n",
    "from torch import Tensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "import torcheval\n",
    "from torcheval.metrics import MulticlassF1Score, Mean\n",
    "import optuna as opt\n",
    "import torchvision as tn\n",
    "import sklearn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as ps\n",
    "import numpy as ny\n",
    "import typing as t\n",
    "import pathlib as pl\n",
    "import matplotlib.pyplot as pt\n",
    "import random as rng\n",
    "from tqdm import tqdm\n",
    "import tqdm as tm\n",
    "from pprint import pprint\n",
    "from git import Repo\n",
    "import lightning as tl\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 7982\n"
     ]
    }
   ],
   "source": [
    "# Add local package to path\n",
    "if (p := pl.Path(so.getcwd(), '..').absolute().as_posix()) not in s.path:\n",
    "    s.path.append(p)\n",
    "\n",
    "# Local imports\n",
    "from gic import *\n",
    "from gic.models.resnet import ResCNN\n",
    "from gic.models.densenet import DenseCNN\n",
    "from gic.models.convnext import ConvNextNet\n",
    "from gic.models.autoencoder import AutoEncoder, UNet\n",
    "from gic.data import load_data, load_batched_data, GICPreprocess, GICPerturb, GICDatasetModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl, test_dl = load_batched_data(DATA_PATH, 'disjoint', gen_torch, pin_memory=pin_memory, num_workers=num_workers, prefetch_factor=prefetch_factor, batch_size=32)\n",
    "tr_perturb = GICPerturb(mask=True, normalize=False)\n",
    "tr_preprocess = GICPreprocess(augment=False, normalize=False)\n",
    "tr_augment = GICPreprocess(augment=True, normalize=False)\n",
    "tr_norm = GICPreprocess(normalize=True, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Denoising UNet AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb as wn\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "\n",
    "class AutoEncoderModule(tl.LightningModule):\n",
    "    logger: WandbLogger\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AutoEncoderModule, self).__init__()\n",
    "        self.save_hyperparameters(kwargs, logger=True)\n",
    "\n",
    "        # Model\n",
    "        self.__loss_l1 = nn.L1Loss()\n",
    "        self.__loss_l2 = nn.MSELoss()\n",
    "        self.__unet = AutoEncoder(**kwargs)\n",
    "\n",
    "        # Data Transformation Operations\n",
    "        self.__tr_mask = GICPerturb(True, False)\n",
    "        self.__tr_norm = GICPreprocess(augment=False, normalize=True)\n",
    "\n",
    "        # Metrics\n",
    "        self.__metric_loss_l1 = Mean(device=self.device)\n",
    "        self.__metric_loss_l2 = Mean(device=self.device)\n",
    "        self.__metric_loss_w = Mean(device=self.device)\n",
    "        self.__top_k = 16\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.__unet(x)[0]\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        self.metrics_to_device()\n",
    "        self.__tb_true: Tensor = torch.stack([self.dataset[i][0] for i in range(self.__top_k)])\n",
    "        self.__tb_true_grid = tn.utils.make_grid(self.__tb_true, nrow=4, pad_value=5)\n",
    "        self.__tb_mask: Tensor = self.__tr_mask(self.__tb_true)\n",
    "        self.__tb_mask_grid = tn.utils.make_grid(self.__tb_mask, nrow=4, pad_value=5)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        self.metrics_reset()\n",
    "\n",
    "    def training_step(self, batch: t.Tuple[Tensor, Tensor], _: t.Any) -> STEP_OUTPUT:\n",
    "        # Retrieve image => apply noise and mask => denoise\n",
    "        X_raw: Tensor = batch[0]\n",
    "        X_true: Tensor = self.__tr_norm(X_raw)\n",
    "        X_noisy: Tensor = self.__tr_norm(self.__tr_mask(X_raw))\n",
    "        X_pred: Tensor = self(X_noisy)\n",
    "\n",
    "        # Reconstruct the original input\n",
    "        loss_l1: Tensor = self.__loss_l1(X_pred, X_true)\n",
    "        loss_l2: Tensor = self.__loss_l2(X_pred, X_true)\n",
    "        loss: Tensor = 0.5 * loss_l1 + 0.5 * loss_l2\n",
    "\n",
    "        # Update current metrics\n",
    "        self.__metric_loss_l1.update(loss_l1.detach())\n",
    "        self.__metric_loss_l2.update(loss_l2.detach())\n",
    "        self.__metric_loss_w.update(loss.detach())\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        self.metrics_log()\n",
    "        b_mask: Tensor = self.__tr_norm(self.__tb_mask.to(self.device))\n",
    "        b_pred: Tensor = self.__tr_norm.denorm(self(b_mask).detach()).cpu()\n",
    "        b_pred_grid = tn.utils.make_grid(b_pred, nrow=4, pad_value=5)\n",
    "        img_a = tn.transforms.transforms.F.to_pil_image(self.__tb_true_grid)\n",
    "        img_b = tn.transforms.transforms.F.to_pil_image(self.__tb_mask_grid)\n",
    "        img_c = tn.transforms.transforms.F.to_pil_image(b_pred_grid)\n",
    "        names = ['Original', 'Noisy', 'Denoised']\n",
    "        images = [img_a, img_b, img_c]\n",
    "        self.logger.log_image(key='Image Denoising', images=images, caption=names)\n",
    "\n",
    "    def on_validation_start(self) -> None:\n",
    "        self.metrics_to_device()\n",
    "        self.__vb_true: Tensor = torch.stack([self.dataset[i][0] for i in range(self.__top_k)])\n",
    "        self.__vb_true_grid = tn.utils.make_grid(self.__vb_true, nrow=4, pad_value=5)\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self.metrics_reset()\n",
    "\n",
    "    def validation_step(self, batch: t.Tuple[Tensor, Tensor], _: t.Any) -> STEP_OUTPUT:\n",
    "        # Retrieve image => apply noise and mask => denoise\n",
    "        X_true: Tensor = self.__tr_norm(batch[0])\n",
    "        X_pred: Tensor = self(X_true)\n",
    "\n",
    "        # Reconstruct the original input\n",
    "        loss_l1: Tensor = self.__loss_l1(X_pred, X_true)\n",
    "        loss_l2: Tensor = self.__loss_l2(X_pred, X_true)\n",
    "        loss: Tensor = 0.5 * loss_l1 + 0.5 * loss_l2\n",
    "\n",
    "        # Update current metrics\n",
    "        self.__metric_loss_l1.update(loss_l1)\n",
    "        self.__metric_loss_l2.update(loss_l2)\n",
    "        self.__metric_loss_w.update(loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        self.metrics_log()\n",
    "        b_true: Tensor = self.__tr_norm(self.__vb_true.to(self.device))\n",
    "        b_pred: Tensor = self.__tr_norm.denorm(self(b_true)).cpu()\n",
    "        b_pred_grid = tn.utils.make_grid(b_pred, nrow=4, pad_value=5)\n",
    "        img_a = tn.transforms.transforms.F.to_pil_image(self.__vb_true_grid)\n",
    "        img_b = tn.transforms.transforms.F.to_pil_image(b_pred_grid)\n",
    "        names = ['Original', 'Reconstructed']\n",
    "        images= [img_a, img_b]\n",
    "        self.logger.log_image(key='Image Reconstruction', images=images, caption=names)\n",
    "\n",
    "    def test_step(self, batch: Tensor, _: t.Any) -> STEP_OUTPUT:\n",
    "        return self(batch)\n",
    "\n",
    "    def predict_step(self, batch: Tensor, _: t.Any) -> Any:\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return om.AdamW(self.parameters(), betas=(0.9, 0.999), lr=6e-4)\n",
    "\n",
    "    def metrics_log(self):\n",
    "        self.log(f'{self.mode}_loss_l1', self.__metric_loss_l1.compute().item())\n",
    "        self.log(f'{self.mode}_loss_l2', self.__metric_loss_l2.compute().item())\n",
    "        self.log(f'{self.mode}_loss_w', self.__metric_loss_w.compute().item())\n",
    "\n",
    "    def metrics_to_device(self):\n",
    "        self.__metric_loss_l1.to(self.device)\n",
    "        self.__metric_loss_l2.to(self.device)\n",
    "        self.__metric_loss_w.to(self.device)\n",
    "\n",
    "    def metrics_reset(self):\n",
    "        self.__metric_loss_l1.reset()\n",
    "        self.__metric_loss_l2.reset()\n",
    "        self.__metric_loss_w.reset()\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> Dataset:\n",
    "        return self.dataloader.dataset\n",
    "\n",
    "    @property\n",
    "    def dataloader(self) -> DataLoader:\n",
    "        if self.trainer.training:\n",
    "            return t.cast(DataLoader, self.trainer.train_dataloader)\n",
    "        elif self.trainer.validating:\n",
    "            return t.cast(DataLoader, self.trainer.val_dataloaders)\n",
    "        elif self.trainer.testing:\n",
    "            return t.cast(DataLoader, self.trainer.test_dataloaders)\n",
    "        else:\n",
    "            return t.cast(DataLoader, self.trainer.predict_dataloaders)\n",
    "\n",
    "    @property\n",
    "    def mode(self):\n",
    "        if self.trainer.training:\n",
    "            return 'train'\n",
    "        elif self.trainer.validating:\n",
    "            return 'valid'\n",
    "        elif self.trainer.testing:\n",
    "            return 'test'\n",
    "        else:\n",
    "            return 'pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wb_logger = WandbLogger(project='Generated Image Classification', name='UNet Test', save_dir=LOG_PATH)\n",
    "trainer = tl.Trainer(max_epochs=20, logger=wb_logger, num_sanity_val_steps=0)\n",
    "model = AutoEncoderModule(chan=64, hchan=256, activ_fn='SiLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33minvokariman\u001b[0m (\u001b[33mcastelvaar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../log/wandb/run-20231206_022633-zncf9ya2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/castelvaar/Generated%20Image%20Classification/runs/zncf9ya2' target=\"_blank\">UNet Test</a></strong> to <a href='https://wandb.ai/castelvaar/Generated%20Image%20Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/castelvaar/Generated%20Image%20Classification' target=\"_blank\">https://wandb.ai/castelvaar/Generated%20Image%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/castelvaar/Generated%20Image%20Classification/runs/zncf9ya2' target=\"_blank\">https://wandb.ai/castelvaar/Generated%20Image%20Classification/runs/zncf9ya2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                        | Type          | Params\n",
      "--------------------------------------------------------------\n",
      "0 | _AutoEncoderModule__loss_l1 | L1Loss        | 0     \n",
      "1 | _AutoEncoderModule__loss_l2 | MSELoss       | 0     \n",
      "2 | _AutoEncoderModule__unet    | AutoEncoder   | 26.1 M\n",
      "3 | _AutoEncoderModule__tr_mask | GICPerturb    | 0     \n",
      "4 | _AutoEncoderModule__tr_norm | GICPreprocess | 0     \n",
      "--------------------------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.208   Total estimated model params size (MB)\n",
      "/home/invokariman/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/invokariman/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b323245bce0400d8ada4c6fb7543c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bafd1c35884941b70b28bdb58dde41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d067738298a4d4bb1bd1a87e2d3c636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc87086966e4835bf32aaccd8bcc6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff024c730204696bd30bd7f2e89f287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ba9fc173904ae183c3f47710f713ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f83f09176ad4f0aa3a7fb0276d51afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7094c885c9394d54b1dbe610b2d67c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33b85fde48a48dab046b47abf726a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f983bf0f2418889ee9d2ceb363e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c3f2b0553941afa5959dffa5a3e676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45f5d2f37bb4f49bf0166bdbfe60feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03ba196ddb34a2a9fb90ee3bab2eab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b356425f014111aab9169506adb59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a4a61dfcd44c358f9da2c95bd038b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adba9259282d430aa21e76e9dc06ade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9c2c12979a4e4eb1e10773d2fc16bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b3b99f9bf24682a42e685a991706c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c943c8535df44b584d26b9f676fb8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dbcc0fbc934b73a955efe7d6ed9d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4c43ce9b304a66b088395d3c29f561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), CKPT_PATH / 'unet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoderModule(\n",
       "  (_AutoEncoderModule__loss_l1): L1Loss()\n",
       "  (_AutoEncoderModule__loss_l2): MSELoss()\n",
       "  (_AutoEncoderModule__unet): AutoEncoder(\n",
       "    (unet): UNet(\n",
       "      (encoder): Encoder(\n",
       "        (in_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (layers): ModuleList(\n",
       "          (0): DownSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (1): LayerNorm((64, 64, 64), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (down_layer): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "          (1): DownSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): LayerNorm((128, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (down_layer): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "          (2): DownSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): LayerNorm((256, 16, 16), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (down_layer): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "          (3): DownSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): LayerNorm((512, 8, 8), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (down_layer): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bottleneck): Bottleneck(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (depthwise_layer): Sequential(\n",
       "              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              (1): LayerNorm((1024, 4, 4), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (pointwise_layer): Conv2d(1024, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activ_fn): SiLU()\n",
       "            (bottleneck_layer): Conv2d(4096, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): UpSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                (1): LayerNorm((1024, 8, 8), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (up_layer): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "          (1): UpSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): LayerNorm((512, 16, 16), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (up_layer): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "          (2): UpSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): LayerNorm((256, 32, 32), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (up_layer): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          )\n",
       "          (3): UpSampleBlock(\n",
       "            (conv_layer): ConvBlock(\n",
       "              (depthwise_layer): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): LayerNorm((128, 64, 64), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (pointwise_layer): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activ_fn): SiLU()\n",
       "              (bottleneck_layer): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (chan_layer): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (out_layer): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_AutoEncoderModule__tr_mask): GICPerturb(\n",
       "    (_GICPerturb__ops): ImageSequential(\n",
       "      (0): ImageSequential(\n",
       "        (ImageSequential_0): ImageSequential(\n",
       "          (RandomErasing_0): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_1): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_2): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_3): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_4): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_5): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_6): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "          (RandomErasing_7): RandomErasing(scale=(0.01, 0.05), resize_to=(0.3, 3.3), value=0.0, p=0.35, p_batch=1.0, same_on_batch=False)\n",
       "        )\n",
       "        (Identity_1): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_AutoEncoderModule__tr_norm): GICPreprocess(\n",
       "    (_GICPreprocess__ops): ImageSequential(\n",
       "      (0): Normalize(p=1.0, p_batch=1.0, same_on_batch=True, mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoderModule(chan=64, hchan=256, activ_fn='SiLU').to(DEVICE)\n",
    "model.load_state_dict(torch.load(CKPT_PATH / 'unet.pt'))\n",
    "model.eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_embd = [model._AutoEncoderModule__unet.forward(tr_norm(X.to(DEVICE)))[1].cpu() for X, _ in iter(train_dl)]\n",
    "tr_embd = torch.cat(tr_embd, dim=0).flatten(start_dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embd = [model._AutoEncoderModule__unet.forward(tr_norm(X.to(DEVICE)))[1].cpu() for X, _ in iter(valid_dl)]\n",
    "val_embd = torch.cat(val_embd, dim=0).flatten(start_dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embd = [model._AutoEncoderModule__unet.forward(tr_norm(X.to(DEVICE)))[1].cpu() for X in iter(test_dl)]\n",
    "test_embd = torch.cat(test_embd, dim=0).flatten(start_dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = torch.cat([y for _, y in iter(train_dl)]).numpy()\n",
    "vl_y = torch.cat([y for _, y in iter(valid_dl)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as kl\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm = LinearSVC(C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm.fit(tr_embd, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "\n",
    "class DenseNetModule(tl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DenseNetModule, self).__init__()\n",
    "\n",
    "        # Model\n",
    "        unet = AutoEncoder(64, 256, 'SiLU')\n",
    "        unet.load_state_dict(torch.load('../ckpt/unet.pt'))\n",
    "        unet.eval().requires_grad_(False)\n",
    "        self.enc = unet.unet.encoder\n",
    "        f_drop = 0.2\n",
    "\n",
    "        self.net_densecnn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Dropout1d(f_drop),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Dropout1d(f_drop),\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(in_features=512, out_features=100)\n",
    "        )\n",
    "        # self.net_densecnn= ConvNextNet(64, 4, conv_dropout=0.1, dense_dropout=0.3, conv_layers=2, dense_features=256, dense_layers=1, patch_reduce=False)\n",
    "        # self.net_densecnn = DenseCNN(**kwargs)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Metrics\n",
    "        self.metric_train_f1_score = MulticlassF1Score(num_classes=CONST_NUM_CLASS, average='macro', device=self.device)\n",
    "        self.metric_valid_f1_score = MulticlassF1Score(num_classes=CONST_NUM_CLASS, average='macro', device=self.device)\n",
    "        self.metric_train_loss = Mean(device=self.device)\n",
    "        self.metric_valid_loss = Mean(device=self.device)\n",
    "        self.save_hyperparameters(kwargs)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        print(x.shape)\n",
    "        _, e = self.enc(x)\n",
    "        return self.net_densecnn(e)\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        self.metric_train_f1_score.to(self.device)\n",
    "        self.metric_train_loss.to(self.device)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        self.metric_train_f1_score.reset()\n",
    "        self.metric_train_loss.reset()\n",
    "\n",
    "    def training_step(self, batch: t.Tuple[Tensor, Tensor], _: t.Any) -> STEP_OUTPUT:\n",
    "        X, y_true = batch\n",
    "        logits: Tensor = self(X)\n",
    "        loss: Tensor = self.loss_fn(logits, y_true)\n",
    "\n",
    "        self.metric_train_loss.update(loss.detach())\n",
    "        self.metric_train_f1_score.update(logits.detach(), y_true)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        self.log('train_f1_score', self.metric_train_f1_score.compute().item())\n",
    "        self.log('train_loss', self.metric_train_loss.compute().item())\n",
    "\n",
    "    def on_validation_start(self) -> None:\n",
    "        self.metric_valid_loss.to(self.device)\n",
    "        self.metric_valid_f1_score.to(self.device)\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self.metric_valid_f1_score.reset()\n",
    "        self.metric_valid_loss.reset()\n",
    "\n",
    "    def validation_step(self, batch: t.Tuple[Tensor, Tensor], _: t.Any) -> STEP_OUTPUT:\n",
    "        X, y_true = batch\n",
    "        logits: Tensor = self(X)\n",
    "        loss: Tensor = self.loss_fn(logits, y_true)\n",
    "\n",
    "        self.metric_valid_loss.update(loss)\n",
    "        self.metric_valid_f1_score.update(logits, y_true)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        self.log('valid_f1_score', self.metric_valid_f1_score.compute().item())\n",
    "        self.log('valid_loss', self.metric_valid_loss.compute().item())\n",
    "\n",
    "    def predict_step(self, batch: Tensor, _: t.Any) -> Any:\n",
    "        return torch.argmax(self(batch), dim=-1)\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        optim = om.AdamW(self.parameters(), betas=(0.9, 0.999), lr=6e-4)\n",
    "        scheduler = om.lr_scheduler.ReduceLROnPlateau(optim, 'max', 0.75, 10, min_lr=2e-4, cooldown=5)\n",
    "        return {\n",
    "            'optimizer': optim,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'train_f1_score',\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "gic_data = GICDatasetModule(DATA_PATH, False, 32, num_workers, prefetch_factor, pin_memory, True, gen_torch)\n",
    "trainer = tl.Trainer(max_epochs=150, enable_checkpointing=True, logger=wb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory ../log/Generated Image Classification/zncf9ya2/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | enc          | Encoder          | 6.6 M \n",
      "1 | net_densecnn | Sequential       | 840 K \n",
      "2 | loss_fn      | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------\n",
      "840 K     Trainable params\n",
      "6.6 M     Non-trainable params\n",
      "7.4 M     Total params\n",
      "29.642    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f670e410ab4c76b99bfe019be2020d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net_densecnn \u001b[39m=\u001b[39m DenseNetModule()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(net_densecnn, datamodule\u001b[39m=\u001b[39;49mgic_data)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1064\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch: t\u001b[39m.\u001b[39mTuple[Tensor, Tensor], _: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m STEP_OUTPUT:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     X, y_true \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     logits: Tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     loss: Tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(logits, y_true)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_valid_loss\u001b[39m.\u001b[39mupdate(loss)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m _, e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/projects/gic/notebooks/main.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet_densecnn(e)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/pooling.py:1194\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49madaptive_avg_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gic-BfYgXNhZ-py3.11/lib/python3.11/site-packages/torch/nn/functional.py:1227\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m   1226\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size)\n\u001b[0;32m-> 1227\u001b[0m _output_size \u001b[39m=\u001b[39m _list_with_default(output_size, \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   1228\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39madaptive_avg_pool2d(\u001b[39minput\u001b[39m, _output_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "net_densecnn = DenseNetModule()\n",
    "trainer.fit(net_densecnn, datamodule=gic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gic_data = GICDatasetModule(DATA_PATH, True, 32, num_workers, prefetch_factor, pin_memory, False, gen_torch)\n",
    "trainer = tl.Trainer(max_epochs=345, enable_checkpointing=False, logger=wb_logger)\n",
    "net_densecnn = DenseNetModule()\n",
    "trainer.fit(net_densecnn, datamodule=gic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat: t.List[Tensor] = t.cast(t.List[Tensor], trainer.predict(net_densecnn, datamodule=gic_data, return_predictions=True))\n",
    "preds = torch.cat(y_hat, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_dl.dataset._GICDataset__data\n",
    "data['Class'] = preds\n",
    "data.to_csv(SUBMISSION_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gic-BfYgXNhZ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
