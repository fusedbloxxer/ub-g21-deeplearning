{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2F1A3tZNFr2"
   },
   "source": [
    "# Text Generation using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "LYDeCjh8M8ac",
    "outputId": "b2d937e7-4e90-4089-bc9c-31c7c4a4b2b4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxasVSynOJ3n"
   },
   "source": [
    "## Download the data\n",
    "\n",
    "The best place to access books that are no longer under Copyright is [Project Gutenberg](https://www.gutenberg.org/). Today we recommend using [Aliceâ€™s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/files/11/11-0.txt) for consistency. Of course you can experiment with other books as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqEtoK_RTlbJ"
   },
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/files/219/219-0.txt'\n",
    "fname = 'heart_of_darkness.txt'\n",
    "\n",
    "if fname not in os.listdir():\n",
    "    urllib.request.urlretrieve(data_url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvBomIM2VpQX"
   },
   "source": [
    "## Load data and create character to integer mappings\n",
    "\n",
    "- Open the text file, read the data then convert it to lowercase letters.\n",
    "- Map each character to a respective number. Keep 2 dictionaries in order to have more easily access to the mappings both ways around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTrVy-7kWc8V"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Characters to integers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1ZroU78YtLk"
   },
   "source": [
    "## Prepare the data\n",
    "- We are \"thinking\" in sequences of 100 characters: 99 characters in the input and 1 in the output.  \n",
    "E.g. for the sequence *\\['h', 'e', 'l', 'l'\\]* as input, we will have *\\['o'\\]* as the expected output.\n",
    "- Reshape X such that it has the shape expected by a LSTM: \\[samples, time steps, features\\].\n",
    "  - samples: number of data points (len(X));\n",
    "  - time steps: number of time-dependent steps that are in a single data point (100);\n",
    "  - features: number of variables for the true value in Y (1).\n",
    "- Scale the values in X to be in \\[0, 1\\].\n",
    "- One-hot encode the true values in Y_modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzMC31hFaGuo"
   },
   "outputs": [],
   "source": [
    "# Initialize the input and output with empty lists\n",
    "\n",
    "for i in range(0, len(text_data) - 100, 1):\n",
    "    # Consider sequences of 99 characters starting from i\n",
    "    \n",
    "    # The 50th character is the label\n",
    "    \n",
    "\n",
    "    # Append to the input the list of ints corresponding to the characters in the current sequence\n",
    "\n",
    "    # Append to the output the int corresponding to the label (as list)\n",
    "\n",
    "# Re-shape the inputs\n",
    "\n",
    "# Scale the inputs\n",
    "\n",
    "# One-hot encode labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_U8znKQesIV"
   },
   "source": [
    "## Define the LSTM model\n",
    "\n",
    "- Instantiate the model: a linear stack of layers.\n",
    "- First layer: LSTM with 256 memory units, input shape from X_new (1st and 2nd). Make sure that this layer returns sequences, such that the next LSTM layer receives sequences and not just random data.\n",
    "- Second layer: dropout 20% of the neurons of the previous layer in order to avoid overfitting.\n",
    "\n",
    "****** \n",
    "Optional:\n",
    "- Third layer: LSTM(256).\n",
    "- Fourth layer: dropout 20% of the neurons.\n",
    "******\n",
    "- Last layer: fully connected with a 'softmax' activation function, and as many neurons as the number of unique characters (the output is one-hot encoded).\n",
    "\n",
    "\n",
    "Compile the model: categorical_crossentropy, adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "pprKER_wev5k",
    "outputId": "8be3699d-15c1-47b7-d217-d04ae080edf8"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "\n",
    "# Add LSTM layer\n",
    "\n",
    "# Add dropout\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM layer\n",
    "\n",
    "# Add dropout\n",
    "\n",
    "# Add a Dense layer\n",
    "\n",
    "# Compile the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMAFfAGchLCd"
   },
   "source": [
    "## Train the model and generate characters\n",
    "\n",
    "Fit the model for over 100 epochs as the batch size is 30 (ideally). In this case, given the time constraints, we are going to use 5 epochs and a batch size of 128. \n",
    "\n",
    "Fix a random seed and start generating characters.  The prediction from the model gives out the character encoding of the predicted character, it is then decoded back to the character value and appended to the pattern.  \n",
    "\n",
    "After enough training time it is going to look like something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "YCqV31YahVVA",
    "outputId": "bf4c7b1c-7206-4e97-9e5f-29519f5bf979"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "n-5fYO89h8SC",
    "outputId": "edae4c70-bd32-42bd-9aa6-916eb71c5cf1"
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "\n",
    "    #predict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXBJeBijfiH0"
   },
   "source": [
    "# Bonus: Words as features\n",
    "\n",
    "Code here:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/ "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
