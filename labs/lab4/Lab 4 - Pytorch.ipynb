{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea95b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6065e",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The best place to access books that are no longer under Copyright is [Project Gutenberg](https://www.gutenberg.org/). Today we recommend using [Alice’s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/files/11/11-0.txt) for consistency. Of course you can experiment with other books as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038f37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/files/219/219-0.txt'\n",
    "fname = 'heart_of_darkness.txt'\n",
    "\n",
    "if fname not in os.listdir():\n",
    "    urllib.request.urlretrieve(data_url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae584b2e",
   "metadata": {},
   "source": [
    "## Load data and create character to integer mappings\n",
    "\n",
    "- Open the text file, read the data then convert it to lowercase letters.\n",
    "- Map each character to a respective number. Keep 2 dictionaries in order to have more easily access to the mappings both ways around.\n",
    "- Transform the data from a list of characters to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569cf4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([58, 55, 57, 20, 12, 1, 29, 10, 48, 20],\n",
       " ['\\ufeff', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "with open(fname, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Preprocess data\n",
    "data = list(data.lower())\n",
    "\n",
    "# Build char-to-int and int-to-char dictionaries\n",
    "c2i = {x: i for i, x in enumerate(set(data))}\n",
    "i2c = {i: x for x, i in c2i.items()}\n",
    "\n",
    "# Transform the data from chars to integers\n",
    "data = [c2i.get(c) for c in data]\n",
    "data[:10], [i2c.get(i) for i in data][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('*', 0),\n",
       "  ('p', 1),\n",
       "  ('n', 2),\n",
       "  ('.', 3),\n",
       "  ('x', 4),\n",
       "  ('6', 5),\n",
       "  ('(', 6),\n",
       "  ('#', 7),\n",
       "  ('’', 8),\n",
       "  ('%', 9)],\n",
       " [(0, '*'),\n",
       "  (1, 'p'),\n",
       "  (2, 'n'),\n",
       "  (3, '.'),\n",
       "  (4, 'x'),\n",
       "  (5, '6'),\n",
       "  (6, '('),\n",
       "  (7, '#'),\n",
       "  (8, '’'),\n",
       "  (9, '%')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(c2i.items())[:10], list(i2c.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b691e4",
   "metadata": {},
   "source": [
    "## Define the datasets and dataloaders\n",
    "- We are \"thinking\" in sequences of 100 characters: 99 characters in the input and 1 in the output.  \n",
    "E.g. for the sequence *\\['h', 'e', 'l', 'l'\\]* as input, we will have *\\['o'\\]* as the expected output.\n",
    "- Each pair (sample, label) from the training dataset will be composed from a sequence of 99 ints and a single integer label\n",
    "- We will keep the first 85% sequences as training data and use the remaining for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbf17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import typing as t\n",
    "\n",
    "# Define datasets\n",
    "class SequenceDataset(data.Dataset):\n",
    "    def __init__(self, data_url: str, fname: str, seq_len: int=99) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Useful props\n",
    "        self.__data_url = data_url\n",
    "        self.__seq_len = seq_len\n",
    "        self.__fname = fname\n",
    "\n",
    "        # Populated through loading\n",
    "        self.c2i: t.Dict[str, int]\n",
    "        self.i2c: t.Dict[int, str]\n",
    "        self.char: bool = False\n",
    "\n",
    "        # Load the data\n",
    "        self.__data = self.__load()\n",
    "\n",
    "    def seq_to_txt(self, seq: t.List[int]) -> str:\n",
    "        return ''.join([self.i2c[i] for i in seq])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        X = self.__data[index:index + self.__seq_len]\n",
    "        y = self.__data[index + self.__seq_len]\n",
    "\n",
    "        if not self.char:\n",
    "            X = torch.tensor(X)\n",
    "            y = torch.tensor(y)\n",
    "            return X, y\n",
    "\n",
    "        return self.seq_to_txt(X), self.i2c[y]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return max(0, len(self.__data) - self.__seq_len)\n",
    "\n",
    "    def __load(self) -> t.List[int]:\n",
    "        # Download it if does not exist\n",
    "        if self.__fname not in os.listdir():\n",
    "            urllib.request.urlretrieve(self.__data_url, self.__fname)\n",
    "\n",
    "        # Load data\n",
    "        with open(self.__fname, 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # Preprocess data\n",
    "        data = list(data.lower())\n",
    "\n",
    "        # Build char-to-int and int-to-char dictionaries\n",
    "        self.c2i = {x: i for i, x in enumerate(set(data))}\n",
    "        self.i2c = {i: x for x, i in c2i.items()}\n",
    "\n",
    "        # Transform the data from chars to integers\n",
    "        return [c2i[c] for c in data]\n",
    "\n",
    "# Create datasets\n",
    "dataset = SequenceDataset(\n",
    "    data_url='https://www.gutenberg.org/files/219/219-0.txt',\n",
    "    fname='heart_of_darkness.txt'\n",
    ")\n",
    "\n",
    "# Split into Train & Validation\n",
    "gen = torch.Generator('cpu')\n",
    "train_d, valid_d  = data.random_split(dataset, [0.85, 0.15], generator=gen)\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define dataloaders\n",
    "batch_size = 128\n",
    "train_dl = data.DataLoader(train_d, batch_size, shuffle=True, generator=gen)\n",
    "valid_dl = data.DataLoader(valid_d, batch_size, shuffle=True, generator=gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6868",
   "metadata": {},
   "source": [
    "## Define a model with\n",
    "- An embedding layer with size 32\n",
    "- Three LSTM layers with a hidden size of 256 and a dropout rate of 20%\n",
    "- A final linear classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, dropout: bool=True):\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.drop = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor, c: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # May or may not apply dropout on the initial hidden state\n",
    "        if self.drop:\n",
    "            h = self.dropout(h)\n",
    "\n",
    "        # Apply the LSTM and return the new states\n",
    "        return self.lstm_cell(x, (h, c))\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_embeddings: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # From int to internal learnable embeddings\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim=32)\n",
    "\n",
    "        # Define a RNN using three LSTM layers, applied one after another\n",
    "        self.rnn = nn.Sequential(\n",
    "            LSTMLayer(input_size=32, hidden_size=256, dropout=False),\n",
    "            LSTMLayer(input_size=32, hidden_size=256, dropout=True),\n",
    "            LSTMLayer(input_size=32, hidden_size=256, dropout=True)\n",
    "        )\n",
    "        self.dense = nn.Linear(in_features=256, out_features=num_embeddings, bias=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea7b59",
   "metadata": {},
   "source": [
    "## Define the training loop and train the model to predict the next character in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8d74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define the training loop and traing the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7c599",
   "metadata": {},
   "source": [
    "## Evaluate the model by generating text\n",
    "\n",
    "- Start with 99 characters (potentially chosen from a text)\n",
    "- Generate a new character using the trained network\n",
    "- Repeat the process by appending the generated character and making a prediction for a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
