{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea95b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import urllib.request\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6065e",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The best place to access books that are no longer under Copyright is [Project Gutenberg](https://www.gutenberg.org/). Today we recommend using [Alice’s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/files/11/11-0.txt) for consistency. Of course you can experiment with other books as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038f37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/files/219/219-0.txt'\n",
    "fname = 'heart_of_darkness.txt'\n",
    "\n",
    "if fname not in os.listdir():\n",
    "    urllib.request.urlretrieve(data_url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae584b2e",
   "metadata": {},
   "source": [
    "## Load data and create character to integer mappings\n",
    "\n",
    "- Open the text file, read the data then convert it to lowercase letters.\n",
    "- Map each character to a respective number. Keep 2 dictionaries in order to have more easily access to the mappings both ways around.\n",
    "- Transform the data from a list of characters to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "569cf4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([22, 43, 41, 13, 25, 2, 39, 45, 29, 13],\n",
       " ['\\ufeff', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "with open(fname, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Preprocess data\n",
    "table = str.maketrans('\\n', ' ')\n",
    "data = list(data.lower().translate(table))\n",
    "\n",
    "# Build char-to-int and int-to-char dictionaries\n",
    "c2i = {x: i for i, x in enumerate(set(data))}\n",
    "i2c = {i: x for x, i in c2i.items()}\n",
    "\n",
    "# Transform the data from chars to integers\n",
    "data = [c2i[c] for c in data]\n",
    "data[:10], [i2c[i] for i in data][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('’', 0),\n",
       "  ('u', 1),\n",
       "  ('p', 2),\n",
       "  ('i', 3),\n",
       "  ('l', 4),\n",
       "  ('_', 5),\n",
       "  (',', 6),\n",
       "  ('4', 7),\n",
       "  ('“', 8),\n",
       "  ('z', 9)],\n",
       " [(0, '’'),\n",
       "  (1, 'u'),\n",
       "  (2, 'p'),\n",
       "  (3, 'i'),\n",
       "  (4, 'l'),\n",
       "  (5, '_'),\n",
       "  (6, ','),\n",
       "  (7, '4'),\n",
       "  (8, '“'),\n",
       "  (9, 'z')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(c2i.items())[:10], list(i2c.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b691e4",
   "metadata": {},
   "source": [
    "## Define the datasets and dataloaders\n",
    "- We are \"thinking\" in sequences of 100 characters: 99 characters in the input and 1 in the output.  \n",
    "E.g. for the sequence *\\['h', 'e', 'l', 'l'\\]* as input, we will have *\\['o'\\]* as the expected output.\n",
    "- Each pair (sample, label) from the training dataset will be composed from a sequence of 99 ints and a single integer label\n",
    "- We will keep the first 85% sequences as training data and use the remaining for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dbf17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import typing as t\n",
    "import string\n",
    "\n",
    "\n",
    "# Define datasets\n",
    "class SequenceDataset(data.Dataset):\n",
    "    def __init__(self, data_url: str, fname: str, seq_len: int=99) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Useful props\n",
    "        self.__data_url = data_url\n",
    "        self.__seq_len = seq_len\n",
    "        self.__fname = fname\n",
    "\n",
    "        # Populated through loading\n",
    "        self.c2i: t.Dict[str, int]\n",
    "        self.i2c: t.Dict[int, str]\n",
    "        self.char: bool = False\n",
    "\n",
    "        # Load the data\n",
    "        self.__data = self.__load()\n",
    "\n",
    "    @property\n",
    "    def units(self) -> int:\n",
    "        return len(self.c2i)\n",
    "\n",
    "    def seq_to_txt(self, seq: t.List[int]) -> str:\n",
    "        return ''.join([self.i2c[i] for i in seq])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        X = self.__data[index:index + self.__seq_len]\n",
    "        y = self.__data[index + self.__seq_len]\n",
    "\n",
    "        if not self.char:\n",
    "            X = torch.tensor(X)\n",
    "            y = torch.tensor(y)\n",
    "            return X, y\n",
    "\n",
    "        return self.seq_to_txt(X), self.i2c[y]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return max(0, len(self.__data) - self.__seq_len)\n",
    "\n",
    "    def __load(self) -> t.List[int]:\n",
    "        # Download it if does not exist\n",
    "        if self.__fname not in os.listdir():\n",
    "            urllib.request.urlretrieve(self.__data_url, self.__fname)\n",
    "\n",
    "        # Load data\n",
    "        with open(self.__fname, 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # Preprocess data\n",
    "        table = str.maketrans('\\n', ' ')\n",
    "        data = list(data.lower().translate(table))\n",
    "\n",
    "        # Build char-to-int and int-to-char dictionaries\n",
    "        self.c2i = {x: i for i, x in enumerate(set(data))}\n",
    "        self.i2c = {i: x for x, i in self.c2i.items()}\n",
    "\n",
    "        # Transform the data from chars to integers\n",
    "        return [self.c2i[c] for c in data]\n",
    "\n",
    "# Create datasets\n",
    "dataset = SequenceDataset(\n",
    "    data_url='https://www.gutenberg.org/cache/epub/65565/pg65565.txt',\n",
    "    fname='Țara mea.txt'\n",
    ")\n",
    "\n",
    "# Split into Train & Validation\n",
    "gen = torch.Generator('cpu')\n",
    "train_d, valid_d  = data.random_split(dataset, [0.85, 0.15], generator=gen)\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define dataloaders\n",
    "batch_size = 128\n",
    "train_dl = data.DataLoader(train_d, batch_size, shuffle=True, generator=gen, num_workers=8, prefetch_factor=2)\n",
    "valid_dl = data.DataLoader(valid_d, batch_size, shuffle=True, generator=gen, num_workers=8, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6868",
   "metadata": {},
   "source": [
    "## Define a model with\n",
    "- An embedding layer with size 32\n",
    "- Three LSTM layers with a hidden size of 256 and a dropout rate of 20%\n",
    "- A final linear classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c45f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, num_embeddings: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # From int to internal learnable embeddings\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim=32)\n",
    "\n",
    "        # Define a RNN using three LSTM layers, applied one after another\n",
    "        self.rnn = nn.LSTM(32, 256, 3, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Apply a classifier on the final hidden state\n",
    "        self.dense = nn.Linear(in_features=256, out_features=num_embeddings, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embeddings(x)\n",
    "        o, (h, c) = self.rnn(x)\n",
    "        x = self.dense(o[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea7b59",
   "metadata": {},
   "source": [
    "## Define the training loop and train the model to predict the next character in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Metrics(t.TypedDict):\n",
    "    accuracy: t.List[float]\n",
    "    loss: t.List[float]\n",
    "\n",
    "\n",
    "class TrainHistory(t.TypedDict):\n",
    "    train: Metrics\n",
    "    valid: Metrics\n",
    "\n",
    "\n",
    "def train_validate(model: nn.Module,\n",
    "                   train_dl: DataLoader,\n",
    "                   valid_dl: DataLoader,\n",
    "                   epochs: int,\n",
    "                   loss_fn: nn.Module,\n",
    "                   optim: torch.optim.Optimizer) -> TrainHistory:\n",
    "    # Track history\n",
    "    history: TrainHistory = {\n",
    "        'train': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        },\n",
    "        'valid': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Do Training & Validation & Testing\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch [%d/%d]' % (epoch + 1, epochs), end=' - ')\n",
    "\n",
    "        ### Training ###\n",
    "        model.train(True)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(train_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prevent grad accumulation\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model.forward(X)\n",
    "            loss: Tensor = loss_fn(logits, y)\n",
    "            y_pred: Tensor = logits.argmax(dim=1).detach()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "            train_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['train']['loss'].append(\n",
    "            torch.mean(torch.tensor(train_loss)).item())\n",
    "        history['train']['accuracy'].append(\n",
    "            (torch.sum(torch.tensor(train_accuracy)) / len(train_accuracy)).item())\n",
    "\n",
    "        ### Validation ###\n",
    "        model.train(False)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        valid_loss = []\n",
    "        valid_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(valid_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                logits = model.forward(X)\n",
    "                loss: Tensor = loss_fn(logits, y)\n",
    "                y_pred: Tensor = logits.argmax(dim=1)\n",
    "\n",
    "            # Track metrics\n",
    "            valid_loss.append(loss.detach().cpu().item())\n",
    "            valid_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['valid']['loss'].append(\n",
    "            torch.mean(torch.tensor(valid_loss)).item())\n",
    "        history['valid']['accuracy'].append(\n",
    "            (torch.sum(torch.tensor(valid_accuracy)) / len(valid_accuracy)).item())\n",
    "\n",
    "        # Inform regarding current metrics\n",
    "        print('t_loss: %f, t_acc: %f, v_loss: %f, v_acc: %f'\n",
    "              % (history['train']['loss'][-1], history['train']['accuracy'][-1], history['valid']['loss'][-1], history['valid']['accuracy'][-1]))\n",
    "\n",
    "    # Output the obtained results so far\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ed8d74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_loss: 2.429425, t_acc: 0.297249, v_loss: 2.089754, v_acc: 0.382244\n",
      "Epoch [2/100] - t_loss: 1.978714, t_acc: 0.409957, v_loss: 1.848271, v_acc: 0.448214\n",
      "Epoch [3/100] - t_loss: 1.788922, t_acc: 0.460497, v_loss: 1.690870, v_acc: 0.487920\n",
      "Epoch [4/100] - t_loss: 1.668142, t_acc: 0.492647, v_loss: 1.614141, v_acc: 0.511128\n",
      "Epoch [5/100] - t_loss: 1.589415, t_acc: 0.513011, v_loss: 1.557657, v_acc: 0.523108\n",
      "Epoch [6/100] - t_loss: 1.531417, t_acc: 0.529143, v_loss: 1.522816, v_acc: 0.532011\n",
      "Epoch [7/100] - t_loss: 1.483238, t_acc: 0.542023, v_loss: 1.498687, v_acc: 0.542175\n",
      "Epoch [8/100] - t_loss: 1.445887, t_acc: 0.550992, v_loss: 1.475052, v_acc: 0.547933\n",
      "Epoch [9/100] - t_loss: 1.414433, t_acc: 0.559077, v_loss: 1.458326, v_acc: 0.552273\n",
      "Epoch [10/100] - t_loss: 1.386680, t_acc: 0.566889, v_loss: 1.444717, v_acc: 0.556060\n",
      "Epoch [11/100] - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab 4 - Pytorch.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# define the training loop and traing the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_validate(model, train_dl, valid_dl, epochs, loss_fn, optim)\n",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab 4 - Pytorch.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m loss: Tensor \u001b[39m=\u001b[39m loss_fn(logits, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m y_pred: Tensor \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab 4 - Pytorch.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     o, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(o[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab4/Lab%204%20-%20Pytorch.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab4-JezBplP3-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab4-JezBplP3-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab4-JezBplP3-py3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Configure the training settings\n",
    "model: RNNModel = RNNModel(num_embeddings=dataset.units)\n",
    "model = model.to(device)\n",
    "optim = Adam(model.parameters(), lr=8e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "\n",
    "# define the training loop and traing the model\n",
    "model.requires_grad_(True)\n",
    "train_validate(model, train_dl, valid_dl, epochs, loss_fn, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7c599",
   "metadata": {},
   "source": [
    "## Evaluate the model by generating text\n",
    "\n",
    "- Start with 99 characters (potentially chosen from a text)\n",
    "- Generate a new character using the trained network\n",
    "- Repeat the process by appending the generated character and making a prediction for a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embeddings): Embedding(78, 32)\n",
       "  (rnn): LSTM(32, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dense): Linear(in_features=256, out_features=78, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(num_embeddings=dataset.units)\n",
    "model.load_state_dict(torch.load('weights.pt'))\n",
    "model = model.to(device)\n",
    "model.requires_grad_(False)\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291579 : ld be clearly marked as such and sent to the project         gutenberg literary archive foundation \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ld be clearly marked as such and sent to the project         gutenberg literary archive foundation edecher work. paragraph domp, your with the plote statos efpiand project gutenberg™. is was empoks, '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Text\n",
    "# \n",
    "start_pos = torch.randint(low=0, high=len(dataset), size=(1,), generator=gen)\n",
    "start, _ = dataset[start_pos]\n",
    "\n",
    "# Show info from dataset\n",
    "dataset.char = True\n",
    "text = t.cast(str, dataset[start_pos][0])\n",
    "print(start_pos.item(), ':', text)\n",
    "dataset.char = False\n",
    "\n",
    "context: Tensor = t.cast(Tensor, start)\n",
    "gen_count = 100\n",
    "k = 25\n",
    "\n",
    "for i in range(gen_count):\n",
    "    # Infer new character\n",
    "    logits: Tensor = model(context.to(device).unsqueeze(0)).cpu()\n",
    "\n",
    "    # Perform Top-K Sampling\n",
    "    top_k = torch.topk(logits, k=k, sorted=False)\n",
    "    top_p = top_k.values.squeeze(0)\n",
    "    top_p = torch.nn.functional.softmax(top_p, 0).cumsum(0)\n",
    "    top_i = torch.searchsorted(top_p, torch.rand(1))\n",
    "\n",
    "    # Obtain prediction index\n",
    "    top_i = int(top_k.indices.squeeze(0)[top_i].item())\n",
    "\n",
    "    # Shift context and use predicted value\n",
    "    context = context.roll(-1)\n",
    "    context[-1] = top_i\n",
    "    text += dataset.i2c[top_i]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([27, 36, 21, 27, 37, 23,  9, 27,  3,  9, 18, 13, 27,  9, 34, 34,  9,  8,\n",
       "          1,  9, 27, 36, 21, 27, 24, 18,  9, 31, 25, 34, 22, 27, 27, 23,  9, 27,\n",
       "          2, 31, 34, 27, 34, 15, 32,  9,  8, 37, 27, 21, 36, 18, 27, 31, 27,  2,\n",
       "         23, 15, 32,  9, 27, 27,  4, 27,  8, 36, 27, 15, 37, 27, 15, 34, 27, 15,\n",
       "         25, 20, 36, 34, 34, 15,  6, 32,  9, 27, 15, 37, 27, 15, 34, 27, 15, 25,\n",
       "         20, 36, 34, 34, 15,  6, 32,  9, 27]),\n",
       " tensor([36, 21, 27, 37, 23,  9, 27,  3,  9, 18, 13, 27,  9, 34, 34,  9,  8,  1,\n",
       "          9, 27, 36, 21, 27, 24, 18,  9, 31, 25, 34, 22, 27, 27, 23,  9, 27,  2,\n",
       "         31, 34, 27, 34, 15, 32,  9,  8, 37, 27, 21, 36, 18, 27, 31, 27,  2, 23,\n",
       "         15, 32,  9, 27, 27,  4, 27,  8, 36, 27, 15, 37, 27, 15, 34, 27, 15, 25,\n",
       "         20, 36, 34, 34, 15,  6, 32,  9, 27, 15, 37, 27, 15, 34, 27, 15, 25, 20,\n",
       "         36, 34, 34, 15,  6, 32,  9, 27, 27]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context, context.roll(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = torch.topk(torch.tensor([0.8, 0.0, 0.05, 0.15]), k=4).values.squeeze(0).cpu()\n",
    "top_k = torch.nn.functional.softmax(top_k, 0).cumsum(0)\n",
    "top_i = torch.searchsorted(top_k, torch.rand(1))\n",
    "top_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1059, 0.1057, 0.1012, 0.0999, 0.0997, 0.0995, 0.0990, 0.0973, 0.0961,\n",
       "        0.0958])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
