{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqFwBVfAkYj4"
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-CnqDF-kPta"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use tf.keras for tasks 1 and 2\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Use keras for Tasks 3, 4, 5.\n",
    "from keras.layers import Dense,GlobalAveragePooling2D, Conv2D, Dropout, MaxPooling2D, Input, Flatten\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_OazODvgwkN"
   },
   "source": [
    "## Task 0: Load the data\n",
    "tf.data.Dataset exposes about 29 preprocessed datasets for developers to easliy load and experiment with test datasets.\n",
    "\n",
    "For the first task, the focus is on tf_flowers. \n",
    "\n",
    "**0.1.** The tf_flowers dataset is not split into train, test and validation. So we need to do that by hand, using **tfds.Split.TRAIN.subsplit** method.\n",
    "\n",
    "**0.2.** Load the dataset using the **tfds.load()** method. Check the documentation for more options on the arguments. Two of the optional arguments that shall be used here are:\n",
    "- **with_info** - is True, which gives the metadata about the dataset.\n",
    "- **as_supervised** - is True, which returns the data and label as a tuple (input, label).\n",
    "\n",
    "**0.3.** Check out the metadata. From this we can see there are a total of 3670 images. After the split, 2950 training images are left.\n",
    "\n",
    "**0.4.** Pre-process the images. Define a function that resize each image to (32, 32), casts it to float and downscales the pixel values by 255. Map this function over the dataset.\n",
    "\n",
    "**0.5.** Use the map function to call the preprocessor on every element of the dataset.\n",
    "\n",
    "**0.6.** A few more operations on the dataset:\n",
    "  - i) Shuffle the training set. Use the SHUFFLE_BUFFER_SIZE constant as argument.\n",
    "  - ii) Batch all the three sets (train, validation and test).\n",
    "  - iii) Prefetch the training data in order to overcome the bottleneck b/w represented by the CPU.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cutw9kNyhcFv"
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH = 32\n",
    "IMG_HEIGHT = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 32\n",
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "SPLIT_WEIGHTS = (8,1,1)\n",
    "\n",
    "# 0.1. Use subsplit from tfds in order to split the tf_flowers dataset\n",
    "splits = # TODO\n",
    "\n",
    "# 0.2. Load the datset\n",
    "(raw_train,raw_val,raw_test),metadata = # TODO\n",
    "\n",
    "# 0.3. Check the metadata to find out the number of samples in each set\n",
    "num_train, num_val, num_test = (\n",
    "                                metadata.splits['train'].num_examples * weight/10 \\\n",
    "                                for weight in SPLIT_WEIGHTS\n",
    "                                )\n",
    "\n",
    "print('Number of training samples {}'.format(num_train))\n",
    "\n",
    "# 0.4. Define a method to pre-process the images\n",
    "def resize_normalize(img, label):\n",
    "    # i) TODO: Use tf.cast to convert the pixels to tf.float32\n",
    "\n",
    "    # ii) TODO: Resize the image to the desired size: 32x32 (use the method from tf)\n",
    "\n",
    "    # iii) TODO: Normalize the pixel values: all channels are b/w 0 and 1\n",
    "\n",
    "    return img, label\n",
    "\n",
    "# 0.5. Map the function defined above over the dataset (on each split)\n",
    "train = # TODO\n",
    "val = # TODO\n",
    "test = # TODO\n",
    "\n",
    "# 0.6. Other preprocessings\n",
    "\n",
    "# i) Shuffle the training size\n",
    "train = # TODO\n",
    "\n",
    "# ii) Batch each of the 3 sets: train, validation and test\n",
    "train = # TODO\n",
    "val = # TODO\n",
    "test = # TODO\n",
    "\n",
    "# iii) Prefetch: AUTOTUNE.\n",
    "train = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fc60yGjO5pg8"
   },
   "source": [
    "## Task 1: Build a base model\n",
    "\n",
    "**1.1.** Define a convolutional neural network with the following architecture:\n",
    "  - **1.1.1.** 3 convolutional layers with 5x5 kernels, same padding and relu activation. After the 1st and 2nd CONV layers, add also MaxPooling, with filters of size 2.\n",
    "  - **1.1.2.** 2 fully connected layers: the first with 128 neurons and relu activation; the second should be used for classification, with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZCBqFaTKTg8"
   },
   "outputs": [],
   "source": [
    "# 1.1. Create the model\n",
    "def create_model():\n",
    "    img_inputs = keras.Input(shape=IMG_SHAPE)\n",
    "\n",
    "    # 1.1.1. Add 3 CONV layers, 5x5, same padding and relu activations; 32, 64, 64 neurons\n",
    "    conv_1 = # TODO\n",
    "    conv_2 = # TODO\n",
    "    conv_3 = # TODO\n",
    "\n",
    "    # 1.1.2. Flatten\n",
    "    flatten = keras.layers.Flatten()(conv_3)\n",
    "\n",
    "    # 1.1.3. Dense layers x 2: (128, relu), (5, softmax)\n",
    "    dense_1 = # TODO\n",
    "    output = # TODO\n",
    "\n",
    "    model = keras.Model(inputs=img_inputs, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "base_model = create_model()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5egBdjmZwOWw"
   },
   "outputs": [],
   "source": [
    "# Number of train, validation and test samples\n",
    "num_train, num_val, num_test = (\n",
    "  metadata.splits['train'].num_examples * weight/10 for weight in SPLIT_WEIGHTS\n",
    ")\n",
    "\n",
    "# Compute the number of steps per epoch such that all the train and validation sets are covered.\n",
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps = round(num_val)//BATCH_SIZE\n",
    "\n",
    "# Print the information about train, test and validation data\n",
    "print('Number of examples in the train set:', num_train)\n",
    "print('Number of examples in the validation set:', num_val)\n",
    "print('Number of examples in the test set:', num_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hFQSvfJgwi3"
   },
   "source": [
    "**1.2.** Train the network for a few iterations and save the model.\n",
    "  - **1.2.1.** Compile the model: adam, sparse_categorical_cross_entropy, accuracy as metric.\n",
    "  - **1.2.2.** Train. For the train and validation data use the repeat() function, to keep spinning the data and reuse it as the steps per epoch cause us to reach the end of the dataset. Please note that specifying the number of train and validation steps is mandatory in this case (when using repeat()).\n",
    "\n",
    "\n",
    "Note: \\\\\n",
    "Please take into account the fact that the amount of data is really small. Do not spend too much time trying to improve the performance of the model, as there are many other tasks left and those will give you more new knowledge to be further used and refined. The aim of this lab is to understand the know-hows of transfer learning and to be able to implement it. Not the performance of the models. Time permitting, you can, of course, do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vW4Q2r7MXpN"
   },
   "outputs": [],
   "source": [
    "# 1.2. Train\n",
    "def train_model(model):\n",
    "    # 1.2.1. TODO: Compile\n",
    "    \n",
    "    # 1.2.2. TODO: Fit\n",
    "    history = # TODO\n",
    "    \n",
    "    return history\n",
    "\n",
    "history = train_model(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "495OAp9l9eiv"
   },
   "source": [
    "## Task 2: Transfering to a new task\n",
    "\n",
    "**2.1.** Load the cifar10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvknh4qV-ytF"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# 2.1. Load cifar10\n",
    "(x_train, y_train), (x_test, y_test) = # TODO\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIn6OK8x_n8n"
   },
   "source": [
    "**2.2.** Keep all the convolutional and max pooling layers from the base model (trained on tf flowers). On top of them we will define two new dense layers and train the entire model on the new dataset.\n",
    "  - **2.2.1.** If your first model followed the specified architecture, the flattening layer applied after the convolutions should be model.layer[-3] to get its output we simply use model.layers[-3].output.\n",
    "  - **2.2.2.** Build 2 new fully connected layers on top of the base model.\n",
    "\n",
    "**2.3.** Define a new model that outputs the result from our newly created layers. Use **tf.keras.model**.\n",
    "\n",
    "**2.4.** Compile the new model: Adam as optimizer, loss sparse categorical crossentropy, accuracy as metric.\n",
    "\n",
    "**2.5.** Train the new model on cifar10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKptYr8vBZI7"
   },
   "outputs": [],
   "source": [
    "# 2.2. Reuse the base model\n",
    "\n",
    "# 2.2.1. Keep only the convolutional and max pooling layers from the base model.\n",
    "conv_output = # TODO\n",
    "\n",
    "# 2.2.2. Build new layers on top of the base model (e.g. (relu, 128 neurons), (10, softmax)\n",
    "fc_1 = # TODO\n",
    "fc_2 = # TODO\n",
    "\n",
    "# 2.3. Define a new model that outputs the result from our newly created layers\n",
    "new_model = # TODO\n",
    "\n",
    "# 2.4. Compile the model: TODO\n",
    "\n",
    "# 2.5. Train on cifar\n",
    "new_history = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07G3BLs5kr1a"
   },
   "source": [
    "## Task 3: Loading Keras Pretrained Models\n",
    "\n",
    "### STEP 1: Build the model\n",
    "\n",
    "**3.1.** Load the ResNet50 model from Keras, pretrained on ImageNet (a dataset with 1000 classes and millions of training examples). \n",
    "\n",
    "**3.2.** The ResNet50 model has 1000 neurons in the last layer (one for each class). We want as many neurons in the last layer of the network as the number of classes in the current problem. Thus we need to discard the 1000 neuron layer and add our own last layer.\n",
    "*Hint*: Set *IncludeTop=False* when importing the model.\n",
    "\n",
    "**3.3.** Add a few **dense layers** such that our model learns more complex functions. The dense layers have **relu as activation** function and the **last layer**, with **as many neurons as the number of classes** has **softmax as activation**.\n",
    "\n",
    "**3.4.** Build a model on the architecture provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPAhS2eD-RYM"
   },
   "outputs": [],
   "source": [
    "# Download the flowers dataset\n",
    "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "zip_file = tf.keras.utils.get_file(origin=_URL, \n",
    "                                   fname=\"flower_photos.tgz\", \n",
    "                                   extract=True)\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')\n",
    "\n",
    "# Some constants\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvg0cAGH8EAM"
   },
   "outputs": [],
   "source": [
    "# 3.1. Import the model, 3.2. without the last layer\n",
    "base_model = #TODO\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "# Add GlobalAveragePooling\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# 3.3. i) Add 2 dense layers: 32, 32, relu\n",
    "x = # TODO\n",
    "x = # TODO\n",
    "\n",
    "# 3.3. ii) Add a final dense layer: 5, softmax\n",
    "predictions = # TODO\n",
    "\n",
    "# 3.4. Build the model\n",
    "model = # TODO\n",
    "\n",
    "# Uncomment below to check the architecture of the model\n",
    "#for i,layer in enumerate(model.layers):\n",
    "#    print(i,layer.name)\n",
    "\n",
    "# TASK 4/5: Play later with freezing/unfreezing some or all the layers of the base network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9NOHDFyIL8G"
   },
   "source": [
    "### Step 2: Load the training data into the ImageDataGenerator\n",
    "\n",
    "ImageDataGeneator is a class from keras, that can help us to train our model. All we need to do is specify the path to our training data and it automatically sends the data for training, in batches. It makes the code much simpler. \n",
    "\n",
    "**3.5.** Use ImageDataGenerator with rescale 1./255 and validation split of 0.2 (80/20).\n",
    "\n",
    "**3.6.** Create the train generator and specify where the train dataset directory, image size, batch size.\n",
    "\n",
    "**3.7.** Create the validation generator with similar approach as the train generator with the flow_from_directory() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5apuCBjDE_5"
   },
   "outputs": [],
   "source": [
    "# 3.5. Rescale the images using ImageDataGenerator\n",
    "datagen = # TODO\n",
    "\n",
    "# 3.6. Use flow_from_directory to specify the base_dir, target_size, batch_size and subset: 'training'\n",
    "train_generator = # TODO\n",
    "\n",
    "# 3.7. For subset: 'validation', use a similar approach as in 3.6.\n",
    "val_generator = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNDw9mzwINTj"
   },
   "source": [
    "### STEP 3: Train on the dataset\n",
    "\n",
    "**3.8.**  Compile the model using **categorical crossentropy**, **accuracy as metric** and **Adam as optimizer**.\n",
    "\n",
    "**3.9.** Train using **train_generator** as **generator**, **step_size_train** as **steps_per_epoch**, and **10 epochs**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J14Kb6ChIOzU"
   },
   "outputs": [],
   "source": [
    "# 3.8. TODO: Compile the model\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 3.9. Train the model\n",
    "history = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6Zu17mpMLfb"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "def plot_lc(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,13.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2e7BIIuMX18"
   },
   "outputs": [],
   "source": [
    "plot_lc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSGBd7i9KLTT"
   },
   "source": [
    "## Task 4: Freezing layers in tf.keras\n",
    "\n",
    "You can specify whether the variables from a layer should be updated or not during the training process by setting the layers , trainable attribute. For instance you can see that all the layers from the loaded model have this paremeter set to True if you iterate through them.\n",
    "\n",
    "```\n",
    "for layer in model.layers:\n",
    "  print(layer.trainable)\n",
    "```\n",
    "\n",
    "If you want to freeze them (i.e. not update their weights during training) you can set the parameter to False.\n",
    "\n",
    "```\n",
    "for layer in model.layers:\n",
    "  layer.trainable = False\n",
    "```\n",
    "\n",
    "Try this in the architecture above and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE51VizFLAaQ"
   },
   "source": [
    "## Task 5: Gradually unfreezing layers\n",
    "\n",
    "For some tasks, the weights learned by the base network are not appropriate enough to constitute a final solution and you might have to update the entire network during training.\n",
    "\n",
    "To avoid training a network that has a highly refined first set of layers and a randomly initialized couple of last layers, a suggested approach is to fine tune the added layers and then gradually unfreeze the layers starting from the top as the training progresses.\n",
    "\n",
    "As a last task you should implement this approach. You can use a smaller pretrained network such as VGG16 or set up a small amount of iterations between each un freezing.\n",
    "\n",
    "You can add these changes directly to Task 3 and check the outcomes."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab3_Trasfer_Learning_SKEL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
