{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6799637",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aad319",
   "metadata": {},
   "source": [
    "In this lab we will make use of pretrained models in order to boost performance on smaller datasets. For this experiment, we will be working with an AlexNet model pretrained on the Imagenet dataset in order to get a good accuracy score on the Caltech 101 dataset.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. In order to perform the experiments, please download in advance the Caltech 101 dataset from https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\n",
    "2. In the working directory please create a folder named 'dataset' and a subfolder named 'caltech101' within it. Extract the dataset in the subfolder. The overall folder structure should look as follows: dataset/caltech101/101_ObjectCategories.\n",
    "3. Install the torchvision module using 'conda install torchvision' if you have not done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30288798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import torch\n",
    "import torchvision\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as t\n",
    "from torch import Tensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "\n",
    "torchvision.set_image_backend('PIL')\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19253b29",
   "metadata": {},
   "source": [
    "Firstly, we will load the AlexNet model architecture using torchvision. All available models with their respective parameters can be found at: https://pytorch.org/vision/stable/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.alexnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81867c00",
   "metadata": {},
   "source": [
    "In the first run we will just load the model architecture, without the pretrained weights. We can visualize the model architecture as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40ab79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21109fdf",
   "metadata": {},
   "source": [
    "Next, we will load the Caltech 101 dataset and apply the neccesary transformations on it. Afterwards, we will split the dataset into train, validation and test.\n",
    "\n",
    "In this block of code, define the dataloaders for train, validation and test and try to iterate through the data. What happens? Try to fix the problem using a lambda transform: https://pytorch.org/vision/stable/transforms.html#generic-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3027bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Resize, Normalize, Lambda, Grayscale\n",
    "from torchvision.models import AlexNet_Weights\n",
    "from torchvision.transforms.v2 import Transform\n",
    "\n",
    "# Use original transformations of AlexNet\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "preprocess: Transform = weights.transforms()\n",
    "transform = Compose([\n",
    "    ToImage(),\n",
    "    ToDtype(dtype=torch.float32, scale=True),\n",
    "    Resize((224, 224)),\n",
    "    Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] != 3 else x),\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for X, y in batch:\n",
    "        images.append(transform(X))\n",
    "        labels.append(y)\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "# Preprocess the dataset using those transforms\n",
    "dataset = torchvision.datasets.Caltech101('./dataset')\n",
    "\n",
    "# Split datasets\n",
    "batch_size= 16\n",
    "n_samples = len(dataset)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# Define dataloaders for train, validation and test\n",
    "# Iterate through the dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=gen, collate_fn=collate_fn, pin_memory=True, pin_memory_device=device.type)\n",
    "valid_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=gen, collate_fn=collate_fn, pin_memory=True, pin_memory_device=device.type)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=gen, collate_fn=collate_fn, pin_memory=True, pin_memory_device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_pin_memory' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nBackendSelect: registered at aten\\src\\ATen\\RegisterBackendSelect.cpp:742 [kernel]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\invokariman\\Documents\\Projects\\Git\\ub-g21-deeplearning\\labs\\lab3\\Lab 03 - Pytorch.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dl))\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:676\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 676\u001b[0m     data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39;49mpin_memory\u001b[39m.\u001b[39;49mpin_memory(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pin_memory_device)\n\u001b[0;32m    677\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:70\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\u001b[39m*\u001b[39m(pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data))\n\u001b[0;32m     69\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[39mfor\u001b[39;49;00m sample \u001b[39min\u001b[39;49;00m data]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[0;32m     72\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\u001b[39m*\u001b[39m(pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data))\n\u001b[0;32m     69\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[0;32m     72\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:58\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpin_memory\u001b[39m(data, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 58\u001b[0m         \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mpin_memory(device)\n\u001b[0;32m     59\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[0;32m     60\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::_pin_memory' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nBackendSelect: registered at aten\\src\\ATen\\RegisterBackendSelect.cpp:742 [kernel]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d6ec1",
   "metadata": {},
   "source": [
    "With the dataset ready, it is now time to adapt the model architecture in order to fit our needs. Define a new classifier for the AlexNet model having the same structure, changing only the number of output neurons to 101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0cf0805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Linear(in_features=4096, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9baa894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Dropout, Linear, ReLU\n",
    "\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3abefe",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Define an Adam optimizer with a learining rate of 1e-4 and a cross entropy loss. Afterwards, train the model for 2 epochs. Note the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1210561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Metrics(t.TypedDict):\n",
    "    accuracy: t.List[float]\n",
    "    loss: t.List[float]\n",
    "\n",
    "\n",
    "class TrainHistory(t.TypedDict):\n",
    "    train: Metrics\n",
    "    valid: Metrics\n",
    "\n",
    "\n",
    "def train_validate(model: nn.Module,\n",
    "                   train_dl: DataLoader,\n",
    "                   valid_dl: DataLoader,\n",
    "                   epochs: int,\n",
    "                   loss_fn: nn.Module,\n",
    "                   optim: torch.optim.Optimizer) -> TrainHistory:\n",
    "    # Track history\n",
    "    history: TrainHistory = {\n",
    "        'train': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        },\n",
    "        'valid': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Do Training & Validation & Testing\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch [%d/%d]' % (epoch + 1, epochs), end=' - ')\n",
    "\n",
    "        ### Training ###\n",
    "        model.train(True)\n",
    "        model.requires_grad_(True)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(train_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prevent grad accumulation\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model.forward(X)\n",
    "            loss: Tensor = loss_fn(logits, y)\n",
    "            y_pred: Tensor = logits.argmax(dim=1).detach()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "            train_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['train']['loss'].append(torch.mean(torch.tensor(train_loss)).item())\n",
    "        history['train']['accuracy'].append((torch.sum(torch.tensor(train_accuracy)) / len(train_accuracy)).item())\n",
    "\n",
    "        ### Validation ###\n",
    "        model.train(False)\n",
    "        model.requires_grad_(False)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        valid_loss = []\n",
    "        valid_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(valid_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model.forward(X)\n",
    "            loss: Tensor = loss_fn(logits, y)\n",
    "            y_pred: Tensor = logits.argmax(dim=1)\n",
    "\n",
    "            # Track metrics\n",
    "            valid_loss.append(loss.detach().cpu().item())\n",
    "            valid_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['valid']['loss'].append(torch.mean(torch.tensor(valid_loss)).item())\n",
    "        history['valid']['accuracy'].append((torch.sum(torch.tensor(valid_accuracy)) / len(valid_accuracy)).item())\n",
    "\n",
    "        # Inform regarding current metrics\n",
    "        print('t_loss: %f, t_acc: %f, v_loss: %f, v_acc: %f'\n",
    "              % (history['train']['loss'][-1], history['train']['accuracy'][-1], history['valid']['loss'][-1], history['valid']['accuracy'][-1]))\n",
    "\n",
    "    # Output the obtained results so far\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3bee427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\invokariman\\Documents\\Projects\\Git\\ub-g21-deeplearning\\labs\\lab3\\Lab 03 - Pytorch.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m history \u001b[39m=\u001b[39m train_validate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_dl\u001b[39m=\u001b[39;49mtrain_dl,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     valid_dl\u001b[39m=\u001b[39;49mvalid_dl,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optim\u001b[39m=\u001b[39;49moptim,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\invokariman\\Documents\\Projects\\Git\\ub-g21-deeplearning\\labs\\lab3\\Lab 03 - Pytorch.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m optim\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Track metrics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/invokariman/Documents/Projects/Git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\invokariman\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lab3-n7gnm7XA-py3.11\\Lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Q: Train the model for 2 epochs using a cross-entropy loss and an Adam optimizer with a lr of 1e-4\n",
    "# Prepare training settings\n",
    "epochs = 2\n",
    "lr_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d9a1e",
   "metadata": {},
   "source": [
    "## Experiments:\n",
    "\n",
    "1. Rerun training (restart kernel and run all cells) but this time, when loading the model in the first block of code, specify 'pretrained = True' in order to make use of the weights pretrained on Imagenet.\n",
    "2. Rerun the code using the pretrained model but this time use a learning rate of 1e-3. What happens?\n",
    "3. Rerun using the pretrained model and a lr of 1e-4 but this time only change the last layer in the model instead of the entire classifier.\n",
    "3. Rerun the code using the pretrained model and a lr of 1e-4. This time, freeze the pretrained layers and only update the new layers for the first epochs. Afterwards, proceed to update the entire model. You can freeze parameters by specifying 'requires_grad = False'.\n",
    "4. Rerun experiment 3 but gradually unfreeze layers instead of unfreezeing the entire model at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5bedd",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b527f35",
   "metadata": {},
   "source": [
    "1. Rerun training (restart kernel and run all cells) but this time, when loading the model in the first block of code, specify 'pretrained = True' in order to make use of the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet_Weights\n",
    "from torchvision.transforms.v2 import Transform\n",
    "\n",
    "\n",
    "# Use original transformations of AlexNet\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "preprocess: Transform = weights.transforms()\n",
    "\n",
    "# Preprocess the dataset using those transforms\n",
    "dataset = torchvision.datasets.Caltech101(\n",
    "    './dataset',\n",
    "    transform = Compose([\n",
    "        ToImage(),\n",
    "        Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] != 3 else x),\n",
    "        Lambda(lambda x: preprocess(x)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Redefine subsets & dataloaders\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8, 0.1, 0.1], gen)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "valid_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b17edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - t_loss: 1.841016, t_acc: 0.582685, v_loss: 0.844043, v_acc: 0.775346\n",
      "Epoch [2/2] - t_loss: 0.528327, t_acc: 0.857678, v_loss: 0.602315, v_acc: 0.839862\n"
     ]
    }
   ],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Prepare training settings\n",
    "epochs = 2\n",
    "lr_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceaf853",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cebc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - t_loss: 4.285237, t_acc: 0.085710, v_loss: 4.198857, v_acc: 0.092166\n",
      "Epoch [2/2] - t_loss: 4.217043, t_acc: 0.086719, v_loss: 4.187852, v_acc: 0.100230\n"
     ]
    }
   ],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Prepare training settings\n",
    "epochs = 2\n",
    "lr_rate = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
