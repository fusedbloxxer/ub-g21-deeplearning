{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6799637",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aad319",
   "metadata": {},
   "source": [
    "In this lab we will make use of pretrained models in order to boost performance on smaller datasets. For this experiment, we will be working with an AlexNet model pretrained on the Imagenet dataset in order to get a good accuracy score on the Caltech 101 dataset.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. In order to perform the experiments, please download in advance the Caltech 101 dataset from https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\n",
    "2. In the working directory please create a folder named 'dataset' and a subfolder named 'caltech101' within it. Extract the dataset in the subfolder. The overall folder structure should look as follows: dataset/caltech101/101_ObjectCategories.\n",
    "3. Install the torchvision module using 'conda install torchvision' if you have not done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30288798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import torch\n",
    "import torchvision\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as t\n",
    "from torch import Tensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "\n",
    "torchvision.set_image_backend('PIL')\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19253b29",
   "metadata": {},
   "source": [
    "Firstly, we will load the AlexNet model architecture using torchvision. All available models with their respective parameters can be found at: https://pytorch.org/vision/stable/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.alexnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81867c00",
   "metadata": {},
   "source": [
    "In the first run we will just load the model architecture, without the pretrained weights. We can visualize the model architecture as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40ab79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21109fdf",
   "metadata": {},
   "source": [
    "Next, we will load the Caltech 101 dataset and apply the neccesary transformations on it. Afterwards, we will split the dataset into train, validation and test.\n",
    "\n",
    "In this block of code, define the dataloaders for train, validation and test and try to iterate through the data. What happens? Try to fix the problem using a lambda transform: https://pytorch.org/vision/stable/transforms.html#generic-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3027bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Resize, Normalize, Lambda, Grayscale\n",
    "from torchvision.models import AlexNet_Weights\n",
    "from torchvision.transforms.v2 import Transform\n",
    "\n",
    "# Use original transformations of AlexNet\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "preprocess: Transform = weights.transforms()\n",
    "transform = Compose([\n",
    "    ToImage(),\n",
    "    ToDtype(dtype=torch.float32, scale=True),\n",
    "    Resize((224, 224)),\n",
    "    Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] != 3 else x),\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for X, y in batch:\n",
    "        images.append(transform(X))\n",
    "        labels.append(y)\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "\n",
    "# Preprocess the dataset using those transforms\n",
    "dataset = torchvision.datasets.Caltech101('./dataset', download=True)\n",
    "\n",
    "# Split datasets\n",
    "batch_size = 16\n",
    "n_samples = len(dataset)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# Speedup settings\n",
    "settings = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': True,\n",
    "    'generator': gen,\n",
    "    'collate_fn': collate_fn,\n",
    "    'pin_memory': True,\n",
    "    'pin_memory_device': device.type,\n",
    "    'num_workers': 8,\n",
    "    'prefetch_factor': 2\n",
    "}\n",
    "\n",
    "# Define dataloaders for train, validation and test\n",
    "# Iterate through the dataloaders\n",
    "train_dl = DataLoader(train_ds, **settings)\n",
    "valid_dl = DataLoader(val_ds, **settings)\n",
    "test_dl = DataLoader(test_ds, **settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d6ec1",
   "metadata": {},
   "source": [
    "With the dataset ready, it is now time to adapt the model architecture in order to fit our needs. Define a new classifier for the AlexNet model having the same structure, changing only the number of output neurons to 101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0cf0805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9baa894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Dropout, Linear, ReLU\n",
    "\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3abefe",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Define an Adam optimizer with a learining rate of 1e-4 and a cross entropy loss. Afterwards, train the model for 2 epochs. Note the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1210561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Metrics(t.TypedDict):\n",
    "    accuracy: t.List[float]\n",
    "    loss: t.List[float]\n",
    "\n",
    "\n",
    "class TrainHistory(t.TypedDict):\n",
    "    train: Metrics\n",
    "    valid: Metrics\n",
    "\n",
    "\n",
    "def train_validate(model: nn.Module,\n",
    "                   train_dl: DataLoader,\n",
    "                   valid_dl: DataLoader,\n",
    "                   epochs: int,\n",
    "                   loss_fn: nn.Module,\n",
    "                   optim: torch.optim.Optimizer) -> TrainHistory:\n",
    "    # Track history\n",
    "    history: TrainHistory = {\n",
    "        'train': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        },\n",
    "        'valid': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Do Training & Validation & Testing\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch [%d/%d]' % (epoch + 1, epochs), end=' - ')\n",
    "\n",
    "        ### Training ###\n",
    "        model.train(True)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(train_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prevent grad accumulation\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model.forward(X)\n",
    "            loss: Tensor = loss_fn(logits, y)\n",
    "            y_pred: Tensor = logits.argmax(dim=1).detach()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "            train_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['train']['loss'].append(torch.mean(torch.tensor(train_loss)).item())\n",
    "        history['train']['accuracy'].append((torch.sum(torch.tensor(train_accuracy)) / len(train_accuracy)).item())\n",
    "\n",
    "        ### Validation ###\n",
    "        model.train(False)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        valid_loss = []\n",
    "        valid_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(valid_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                logits = model.forward(X)\n",
    "                loss: Tensor = loss_fn(logits, y)\n",
    "                y_pred: Tensor = logits.argmax(dim=1)\n",
    "\n",
    "            # Track metrics\n",
    "            valid_loss.append(loss.detach().cpu().item())\n",
    "            valid_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['valid']['loss'].append(torch.mean(torch.tensor(valid_loss)).item())\n",
    "        history['valid']['accuracy'].append((torch.sum(torch.tensor(valid_accuracy)) / len(valid_accuracy)).item())\n",
    "\n",
    "        # Inform regarding current metrics\n",
    "        print('t_loss: %f, t_acc: %f, v_loss: %f, v_acc: %f'\n",
    "              % (history['train']['loss'][-1], history['train']['accuracy'][-1], history['valid']['loss'][-1], history['valid']['accuracy'][-1]))\n",
    "\n",
    "    # Output the obtained results so far\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bee427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_loss: 3.794581, t_acc: 0.189283, v_loss: 3.359960, v_acc: 0.266129\n",
      "Epoch [2/10] - t_loss: 2.977539, t_acc: 0.359695, v_loss: 2.769620, v_acc: 0.369816\n",
      "Epoch [3/10] - t_loss: 2.352770, t_acc: 0.457361, v_loss: 2.314114, v_acc: 0.450461\n",
      "Epoch [4/10] - t_loss: 1.917602, t_acc: 0.540334, v_loss: 1.940583, v_acc: 0.538018\n",
      "Epoch [5/10] - t_loss: 1.592094, t_acc: 0.608902, v_loss: 1.742621, v_acc: 0.580645\n",
      "Epoch [6/10] - t_loss: 1.296296, t_acc: 0.666955, v_loss: 1.652966, v_acc: 0.601382\n",
      "Epoch [7/10] - t_loss: 1.065086, t_acc: 0.718093, v_loss: 1.618707, v_acc: 0.604839\n",
      "Epoch [8/10] - t_loss: 0.819117, t_acc: 0.771248, v_loss: 1.655721, v_acc: 0.617512\n",
      "Epoch [9/10] - t_loss: 0.617592, t_acc: 0.827715, v_loss: 1.655025, v_acc: 0.610599\n",
      "Epoch [10/10] - t_loss: 0.460418, t_acc: 0.863872, v_loss: 1.650661, v_acc: 0.660138\n"
     ]
    }
   ],
   "source": [
    "# Q: Train the model for 2 epochs using a cross-entropy loss and an Adam optimizer with a lr of 1e-4\n",
    "# Prepare training settings\n",
    "epochs = 10\n",
    "lr_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "p1_history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d9a1e",
   "metadata": {},
   "source": [
    "## Experiments:\n",
    "\n",
    "1. Rerun training (restart kernel and run all cells) but this time, when loading the model in the first block of code, specify 'pretrained = True' in order to make use of the weights pretrained on Imagenet.\n",
    "2. Rerun the code using the pretrained model but this time use a learning rate of 1e-3. What happens?\n",
    "3. Rerun using the pretrained model and a lr of 1e-4 but this time only change the last layer in the model instead of the entire classifier.\n",
    "4. Rerun the code using the pretrained model and a lr of 1e-4. This time, freeze the pretrained layers and only update the new layers for the first epochs. Afterwards, proceed to update the entire model. You can freeze parameters by specifying 'requires_grad = False'.\n",
    "5. Rerun experiment 3 but gradually unfreeze layers instead of unfreezeing the entire model at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5340bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet_Weights\n",
    "from torchvision.transforms.v2 import Transform\n",
    "\n",
    "\n",
    "# Use original transformations of AlexNet\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "preprocess: Transform = weights.transforms()\n",
    "\n",
    "# Preprocess the dataset using those transforms\n",
    "dataset = torchvision.datasets.Caltech101(\n",
    "    './dataset',\n",
    "    transform = Compose([\n",
    "        ToImage(),\n",
    "        Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] != 3 else x),\n",
    "        Lambda(lambda x: preprocess(x)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Redefine subsets & dataloaders\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8, 0.1, 0.1], gen)\n",
    "train_dl = DataLoader(train_ds, **settings)\n",
    "valid_dl = DataLoader(val_ds, **settings)\n",
    "test_dl = DataLoader(test_ds, **settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5bedd",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b527f35",
   "metadata": {},
   "source": [
    "Rerun training (restart kernel and run all cells) but this time, when loading the model in the first block of code, specify 'pretrained = True' in order to make use of the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b17edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - t_loss: 1.822363, t_acc: 0.585278, v_loss: 0.811106, v_acc: 0.798387\n",
      "Epoch [2/10] - t_loss: 0.544301, t_acc: 0.855085, v_loss: 0.565776, v_acc: 0.857143\n",
      "Epoch [3/10] - t_loss: 0.241973, t_acc: 0.934457, v_loss: 0.612962, v_acc: 0.846774\n",
      "Epoch [4/10] - t_loss: 0.170993, t_acc: 0.953616, v_loss: 0.571546, v_acc: 0.868664\n",
      "Epoch [5/10] - t_loss: 0.123834, t_acc: 0.963411, v_loss: 0.607776, v_acc: 0.866359\n",
      "Epoch [6/10] - t_loss: 0.093824, t_acc: 0.973351, v_loss: 0.584042, v_acc: 0.866359\n",
      "Epoch [7/10] - t_loss: 0.101693, t_acc: 0.972198, v_loss: 0.714532, v_acc: 0.847926\n",
      "Epoch [8/10] - t_loss: 0.069689, t_acc: 0.979833, v_loss: 0.815824, v_acc: 0.835253\n",
      "Epoch [9/10] - t_loss: 0.104316, t_acc: 0.971766, v_loss: 0.504716, v_acc: 0.868664\n",
      "Epoch [10/10] - t_loss: 0.090155, t_acc: 0.977384, v_loss: 0.552409, v_acc: 0.866359\n"
     ]
    }
   ],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Prepare training settings\n",
    "lr_rate = 1e-4\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "p1_history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceaf853",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57689475",
   "metadata": {},
   "source": [
    "Rerun the code using the pretrained model but this time use a learning rate of 1e-3. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08cebc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - t_loss: 4.297482, t_acc: 0.084270, v_loss: 4.232741, v_acc: 0.081797\n",
      "Epoch [2/10] - t_loss: 4.209764, t_acc: 0.091760, v_loss: 4.225294, v_acc: 0.095622\n",
      "Epoch [3/10] - t_loss: 4.206759, t_acc: 0.092625, v_loss: 4.224267, v_acc: 0.081797\n",
      "Epoch [4/10] - t_loss: 4.204620, t_acc: 0.091616, v_loss: 4.229794, v_acc: 0.095622\n",
      "Epoch [5/10] - t_loss: 4.203608, t_acc: 0.092913, v_loss: 4.213019, v_acc: 0.095622\n",
      "Epoch [6/10] - t_loss: 4.203966, t_acc: 0.086863, v_loss: 4.218480, v_acc: 0.081797\n",
      "Epoch [7/10] - t_loss: 4.201900, t_acc: 0.091760, v_loss: 4.211495, v_acc: 0.081797\n",
      "Epoch [8/10] - t_loss: 4.200199, t_acc: 0.088159, v_loss: 4.223351, v_acc: 0.081797\n",
      "Epoch [9/10] - t_loss: 4.198719, t_acc: 0.091904, v_loss: 4.232707, v_acc: 0.081797\n",
      "Epoch [10/10] - t_loss: 4.199227, t_acc: 0.090608, v_loss: 4.230655, v_acc: 0.081797\n"
     ]
    }
   ],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Prepare training settings\n",
    "lr_rate = 1e-3\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "p1_history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9372dcf",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0651afa",
   "metadata": {},
   "source": [
    "Rerun using the pretrained model and a lr of 1e-4 but this time only change the last layer in the model instead of the entire classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2df7da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - t_loss: 1.210781, t_acc: 0.702391, v_loss: 0.571359, v_acc: 0.838710\n",
      "Epoch [2/10] - t_loss: 0.227575, t_acc: 0.933449, v_loss: 0.534217, v_acc: 0.854839\n",
      "Epoch [3/10] - t_loss: 0.099769, t_acc: 0.971766, v_loss: 0.510533, v_acc: 0.862903\n",
      "Epoch [4/10] - t_loss: 0.066575, t_acc: 0.979833, v_loss: 0.456373, v_acc: 0.883641\n",
      "Epoch [5/10] - t_loss: 0.053408, t_acc: 0.984875, v_loss: 0.524013, v_acc: 0.880184\n",
      "Epoch [6/10] - t_loss: 0.053102, t_acc: 0.984443, v_loss: 0.525328, v_acc: 0.884793\n",
      "Epoch [7/10] - t_loss: 0.050242, t_acc: 0.985019, v_loss: 0.535523, v_acc: 0.882488\n",
      "Epoch [8/10] - t_loss: 0.062937, t_acc: 0.980985, v_loss: 0.612010, v_acc: 0.858295\n",
      "Epoch [9/10] - t_loss: 0.045422, t_acc: 0.986027, v_loss: 0.633534, v_acc: 0.862903\n",
      "Epoch [10/10] - t_loss: 0.043690, t_acc: 0.986603, v_loss: 0.496371, v_acc: 0.882488\n"
     ]
    }
   ],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Change only the last layer of the network\n",
    "model.classifier[-1] = nn.Linear(in_features=4096, out_features=101, bias=True)\n",
    "\n",
    "# Prepare training settings\n",
    "lr_rate = 1e-4\n",
    "optim = torch.optim.Adam(model.classifier.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "p1_history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aead36",
   "metadata": {},
   "source": [
    "Rerun the code using the pretrained model and a lr of 1e-4. This time, freeze the pretrained layers and only update the new layers for the first epochs. Afterwards, proceed to update the entire model. You can freeze parameters by specifying 'requires_grad = False'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - t_loss: 1.705078, t_acc: 0.616825, v_loss: 0.705593, v_acc: 0.813364\n",
      "Epoch [2/3] - t_loss: 0.463794, t_acc: 0.871363, v_loss: 0.578746, v_acc: 0.842166\n",
      "Epoch [3/3] - t_loss: 0.235555, t_acc: 0.937626, v_loss: 0.521116, v_acc: 0.855991\n",
      "Epoch [1/7] - t_loss: 0.263022, t_acc: 0.926102, v_loss: 0.553663, v_acc: 0.853687\n",
      "Epoch [2/7] - t_loss: 0.173932, t_acc: 0.949006, v_loss: 0.597264, v_acc: 0.836406\n",
      "Epoch [3/7] - t_loss: 0.129096, t_acc: 0.963123, v_loss: 0.529493, v_acc: 0.861751\n",
      "Epoch [4/7] - t_loss: 0.132028, t_acc: 0.962835, v_loss: 0.613628, v_acc: 0.864055\n",
      "Epoch [5/7] - t_loss: 0.095486, t_acc: 0.972054, v_loss: 0.553972, v_acc: 0.875576\n",
      "Epoch [6/7] - t_loss: 0.067284, t_acc: 0.979977, v_loss: 0.571430, v_acc: 0.861751\n",
      "Epoch [7/7] - t_loss: 0.067713, t_acc: 0.981706, v_loss: 0.584910, v_acc: 0.868664\n"
     ]
    }
   ],
   "source": [
    "# Init the model from the pretrained weights\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Freeze the backbone initially\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# Replace the top classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "lr = 1e-4\n",
    "model = model.to(device)\n",
    "optim = torch.optim.Adam(model.classifier.parameters(), lr)\n",
    "\n",
    "p1_ratio = 0.3\n",
    "p2_ratio = 1 - p1_ratio\n",
    "assert 1.0 - (p1_ratio + p2_ratio) <= 1e-5\n",
    "\n",
    "p1_epochs: int = int(np.floor(p1_ratio * epochs))\n",
    "p2_epochs: int = int(np.floor(p2_ratio * epochs))\n",
    "assert p1_epochs + p2_epochs == epochs\n",
    "\n",
    "# Phase 1 - Train the classifier\n",
    "p1_history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=p1_epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")\n",
    "\n",
    "# Consider all params for optimization\n",
    "optim.add_param_group({\n",
    "    'params': model.features.parameters(),\n",
    "})\n",
    "\n",
    "# Unfreeze the whole model\n",
    "model.requires_grad_(True)\n",
    "\n",
    "# Phase 2 - Train the whole network\n",
    "p2_history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=p2_epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Rerun experiment 3 but gradually unfreeze layers instead of unfreezeing the entire model at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 1.617894, t_acc: 0.630510, v_loss: 0.801819, v_acc: 0.828341\n",
      "Epoch [2/2] - t_loss: 0.505628, t_acc: 0.877989, v_loss: 0.606884, v_acc: 0.850230\n",
      "['classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.321002, t_acc: 0.903630, v_loss: 0.538197, v_acc: 0.850230\n",
      "Epoch [2/2] - t_loss: 0.108363, t_acc: 0.968165, v_loss: 0.532982, v_acc: 0.864055\n",
      "['classifier.1.weight:True', 'classifier.1.bias:True', 'classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.156007, t_acc: 0.957073, v_loss: 0.627823, v_acc: 0.857143\n",
      "Epoch [2/2] - t_loss: 0.122495, t_acc: 0.962403, v_loss: 0.579784, v_acc: 0.854839\n",
      "['features.10.weight:True', 'features.10.bias:True', 'classifier.1.weight:True', 'classifier.1.bias:True', 'classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.085865, t_acc: 0.972198, v_loss: 0.584050, v_acc: 0.866359\n",
      "Epoch [2/2] - t_loss: 0.051571, t_acc: 0.983866, v_loss: 0.580650, v_acc: 0.874424\n",
      "['features.8.weight:True', 'features.8.bias:True', 'features.10.weight:True', 'features.10.bias:True', 'classifier.1.weight:True', 'classifier.1.bias:True', 'classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.076146, t_acc: 0.977096, v_loss: 0.631596, v_acc: 0.861751\n",
      "Epoch [2/2] - t_loss: 0.079598, t_acc: 0.975655, v_loss: 0.690094, v_acc: 0.846774\n",
      "['features.6.weight:True', 'features.6.bias:True', 'features.8.weight:True', 'features.8.bias:True', 'features.10.weight:True', 'features.10.bias:True', 'classifier.1.weight:True', 'classifier.1.bias:True', 'classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.113935, t_acc: 0.967300, v_loss: 0.681109, v_acc: 0.867512\n",
      "Epoch [2/2] - t_loss: 0.087838, t_acc: 0.972630, v_loss: 0.657737, v_acc: 0.862903\n",
      "['features.3.weight:True', 'features.3.bias:True', 'features.6.weight:True', 'features.6.bias:True', 'features.8.weight:True', 'features.8.bias:True', 'features.10.weight:True', 'features.10.bias:True', 'classifier.1.weight:True', 'classifier.1.bias:True', 'classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.112357, t_acc: 0.967156, v_loss: 0.641344, v_acc: 0.861751\n",
      "Epoch [2/2] - t_loss: 0.085375, t_acc: 0.974359, v_loss: 0.710366, v_acc: 0.859447\n",
      "['features.0.weight:True', 'features.0.bias:True', 'features.3.weight:True', 'features.3.bias:True', 'features.6.weight:True', 'features.6.bias:True', 'features.8.weight:True', 'features.8.bias:True', 'features.10.weight:True', 'features.10.bias:True', 'classifier.1.weight:True', 'classifier.1.bias:True', 'classifier.4.weight:True', 'classifier.4.bias:True', 'classifier.6.weight:True', 'classifier.6.bias:True']\n",
      "Epoch [1/2] - t_loss: 0.094737, t_acc: 0.970902, v_loss: 0.662735, v_acc: 0.845622\n",
      "Epoch [2/2] - t_loss: 0.082394, t_acc: 0.975944, v_loss: 0.629333, v_acc: 0.873272\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.alexnet(weights=weights)\n",
    "model.classifier[-1] = nn.Linear(in_features=4096, out_features=101, bias=True)\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# Prepare training settings\n",
    "lr_rate = 1e-4\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Get all layes in reverse order (bias included)\n",
    "param_names = list(map(lambda x: x[0], reversed(list(model.named_parameters()))))\n",
    "param_groups = list(zip(*(iter(param_names),) * 2))\n",
    "history = []\n",
    "epochs_per_layer = 2\n",
    "\n",
    "for bias, weight in param_groups:\n",
    "    # Unfreeze layer by layer\n",
    "    model.get_parameter(bias).requires_grad_(True)\n",
    "    model.get_parameter(weight).requires_grad_(True)\n",
    "\n",
    "    # See what parameters are currently trained\n",
    "    print(list(filter(lambda x: 'true' in x.lower(), map(lambda x: f'{x[0]}:{x[1].requires_grad}', model.named_parameters()))))\n",
    "\n",
    "    # Train model partially\n",
    "    h = train_validate(\n",
    "        model=model,\n",
    "        train_dl=train_dl,\n",
    "        valid_dl=valid_dl,\n",
    "        epochs=epochs_per_layer,\n",
    "        loss_fn=loss_fn,\n",
    "        optim=optim,\n",
    "    )\n",
    "\n",
    "    # Keep results\n",
    "    history.append(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
