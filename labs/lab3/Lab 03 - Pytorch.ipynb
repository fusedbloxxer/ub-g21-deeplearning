{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6799637",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aad319",
   "metadata": {},
   "source": [
    "In this lab we will make use of pretrained models in order to boost performance on smaller datasets. For this experiment, we will be working with an AlexNet model pretrained on the Imagenet dataset in order to get a good accuracy score on the Caltech 101 dataset.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. In order to perform the experiments, please download in advance the Caltech 101 dataset from https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\n",
    "2. In the working directory please create a folder named 'dataset' and a subfolder named 'caltech101' within it. Extract the dataset in the subfolder. The overall folder structure should look as follows: dataset/caltech101/101_ObjectCategories.\n",
    "3. Install the torchvision module using 'conda install torchvision' if you have not done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30288798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import torch\n",
    "import torchvision\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as t\n",
    "from torch import Tensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19253b29",
   "metadata": {},
   "source": [
    "Firstly, we will load the AlexNet model architecture using torchvision. All available models with their respective parameters can be found at: https://pytorch.org/vision/stable/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.alexnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81867c00",
   "metadata": {},
   "source": [
    "In the first run we will just load the model architecture, without the pretrained weights. We can visualize the model architecture as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40ab79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21109fdf",
   "metadata": {},
   "source": [
    "Next, we will load the Caltech 101 dataset and apply the neccesary transformations on it. Afterwards, we will split the dataset into train, validation and test.\n",
    "\n",
    "In this block of code, define the dataloaders for train, validation and test and try to iterate through the data. What happens? Try to fix the problem using a lambda transform: https://pytorch.org/vision/stable/transforms.html#generic-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3027bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Resize, Normalize, Lambda\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.Caltech101(\n",
    "    './dataset',\n",
    "    transform = Compose([\n",
    "        ToImage(),\n",
    "        ToDtype(torch.float, scale=True),\n",
    "        Resize((224, 224)),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "batch_size= 16\n",
    "n_samples = len(dataset)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8, 0.1, 0.1], gen)\n",
    "\n",
    "# define dataloaders for train, validation and test\n",
    "# iterate through the dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "valid_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d6ec1",
   "metadata": {},
   "source": [
    "With the dataset ready, it is now time to adapt the model architecture in order to fit our needs. Define a new classifier for the AlexNet model having the same structure, changing only the number of output neurons to 101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0cf0805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9baa894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Dropout, Linear, ReLU\n",
    "\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3abefe",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Define an Adam optimizer with a learining rate of 1e-4 and a cross entropy loss. Afterwards, train the model for 2 epochs. Note the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1210561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Metrics(t.TypedDict):\n",
    "    accuracy: t.List[float]\n",
    "    loss: t.List[float]\n",
    "\n",
    "\n",
    "class TrainHistory(t.TypedDict):\n",
    "    train: Metrics\n",
    "    valid: Metrics\n",
    "\n",
    "\n",
    "def train_validate(model: nn.Module,\n",
    "                   train_dl: DataLoader,\n",
    "                   valid_dl: DataLoader,\n",
    "                   epochs: int,\n",
    "                   loss_fn: nn.Module,\n",
    "                   optim: torch.optim.Optimizer) -> TrainHistory:\n",
    "    # Track history\n",
    "    history: TrainHistory = {\n",
    "        'train': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        },\n",
    "        'valid': {\n",
    "            'accuracy': [],\n",
    "            'loss': [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Do Training & Validation & Testing\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch [%d/%d]' % (epoch + 1, epochs), end=' - ')\n",
    "\n",
    "        ### Training ###\n",
    "        model.train(True)\n",
    "        model.requires_grad_(True)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(train_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prevent grad accumulation\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model.forward(X)\n",
    "            loss: Tensor = loss_fn(logits, y)\n",
    "            y_pred: Tensor = logits.argmax(dim=1).detach()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "            train_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['train']['loss'].append(torch.mean(torch.tensor(train_loss)).item())\n",
    "        history['train']['accuracy'].append((torch.sum(torch.tensor(train_accuracy)) / len(train_accuracy)).item())\n",
    "\n",
    "        ### Validation ###\n",
    "        model.train(False)\n",
    "        model.requires_grad_(False)\n",
    "\n",
    "        # Track across a single epoch\n",
    "        valid_loss = []\n",
    "        valid_accuracy = []\n",
    "\n",
    "        for b, (X, y) in enumerate(valid_dl):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model.forward(X)\n",
    "            loss: Tensor = loss_fn(logits, y)\n",
    "            y_pred: Tensor = logits.argmax(dim=1)\n",
    "\n",
    "            # Track metrics\n",
    "            valid_loss.append(loss.detach().cpu().item())\n",
    "            valid_accuracy.extend((y_pred == y).detach().cpu().tolist())\n",
    "\n",
    "        # Aggregate training results\n",
    "        history['valid']['loss'].append(torch.mean(torch.tensor(valid_loss)).item())\n",
    "        history['valid']['accuracy'].append((torch.sum(torch.tensor(valid_accuracy)) / len(valid_accuracy)).item())\n",
    "\n",
    "        # Inform regarding current metrics\n",
    "        print('t_loss: %f, t_acc: %f, v_loss: %f, v_acc: %f'\n",
    "              % (history['train']['loss'][-1], history['train']['accuracy'][-1], history['valid']['loss'][-1], history['valid']['accuracy'][-1]))\n",
    "\n",
    "    # Output the obtained results so far\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bee427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - t_loss: 3.627621, t_acc: 0.232642, v_loss: 2.983734, v_acc: 0.342166\n",
      "Epoch [2/2] - t_loss: 2.666375, t_acc: 0.415298, v_loss: 2.404627, v_acc: 0.468894\n"
     ]
    }
   ],
   "source": [
    "# Q: Train the model for 2 epochs using a cross-entropy loss and an Adam optimizer with a lr of 1e-4\n",
    "# Prepare training settings\n",
    "epochs = 2\n",
    "lr_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d9a1e",
   "metadata": {},
   "source": [
    "## Experiments:\n",
    "\n",
    "1. Rerun training (restart kernel and run all cells) but this time, when loading the model in the first block of code, specify 'pretrained = True' in order to make use of the weights pretrained on Imagenet.\n",
    "2. Rerun the code using the pretrained model but this time use a learning rate of 1e-3. What happens?\n",
    "3. Rerun using the pretrained model and a lr of 1e-4 but this time only change the last layer in the model instead of the entire classifier.\n",
    "3. Rerun the code using the pretrained model and a lr of 1e-4. This time, freeze the pretrained layers and only update the new layers for the first epochs. Afterwards, proceed to update the entire model. You can freeze parameters by specifying 'requires_grad = False'.\n",
    "4. Rerun experiment 3 but gradually unfreeze layers instead of unfreezeing the entire model at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5bedd",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b527f35",
   "metadata": {},
   "source": [
    "1. Rerun training (restart kernel and run all cells) but this time, when loading the model in the first block of code, specify 'pretrained = True' in order to make use of the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5340bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet_Weights\n",
    "from torchvision.transforms.v2 import Transform\n",
    "\n",
    "\n",
    "# Use original transformations of AlexNet\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "preprocess: Transform = weights.transforms()\n",
    "\n",
    "# Preprocess the dataset using those transforms\n",
    "dataset = torchvision.datasets.Caltech101(\n",
    "    './dataset',\n",
    "    transform = Lambda(lambda x: preprocess(x))\n",
    ")\n",
    "\n",
    "# Redefine subsets & dataloaders\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8, 0.1, 0.1], gen)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "valid_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True, generator=gen) ## todo define custom collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82b17edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab 03 - Pytorch.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m history \u001b[39m=\u001b[39m train_validate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     train_dl\u001b[39m=\u001b[39;49mtrain_dl,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     valid_dl\u001b[39m=\u001b[39;49mvalid_dl,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     optim\u001b[39m=\u001b[39;49moptim,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab 03 - Pytorch.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m train_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m train_accuracy \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m b, (X, y) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(train_dl):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     X, y \u001b[39m=\u001b[39;49m X\u001b[39m.\u001b[39;49mto(device), y\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# Prevent grad accumulation\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m indices]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/datasets/caltech.py:112\u001b[0m, in \u001b[0;36mCaltech101.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(target) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(target) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m target[\u001b[39m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;49;00m needs_transform \u001b[39melse\u001b[39;49;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;49;00m (inpt, needs_transform) \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;00m needs_transform \u001b[39melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_misc.py:41\u001b[0m, in \u001b[0;36mLambda._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, inpt: Any, params: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inpt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtypes):\n\u001b[0;32m---> 41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlambd(inpt)\n\u001b[1;32m     42\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m inpt\n",
      "\u001b[1;32m/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab 03 - Pytorch.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preprocess: Transform \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mtransforms()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Preprocess the dataset using those transforms\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dataset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCaltech101(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m./dataset\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     transform \u001b[39m=\u001b[39m Lambda(\u001b[39mlambda\u001b[39;00m x: preprocess(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Redefine subsets & dataloaders\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/invokariman/Projects/git/ub-g21-deeplearning/labs/lab3/Lab%2003%20-%20Pytorch.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_ds, val_ds, test_ds \u001b[39m=\u001b[39m random_split(dataset, [\u001b[39m0.8\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.1\u001b[39m], gen)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/transforms/_presets.py:63\u001b[0m, in \u001b[0;36mImageClassification.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     61\u001b[0m     img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpil_to_tensor(img)\n\u001b[1;32m     62\u001b[0m img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mconvert_image_dtype(img, torch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m---> 63\u001b[0m img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mnormalize(img, mean\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, std\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd)\n\u001b[1;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lab3-OcZGOJIv-py3.11/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    927\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 928\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]"
     ]
    }
   ],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Prepare training settings\n",
    "epochs = 2\n",
    "lr_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceaf853",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cebc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pretrained model\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "\n",
    "# Create a new classifier similar to AlexNet\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=9216, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Dropout(p=0.5, inplace=False),\n",
    "    Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(in_features=4096, out_features=101, bias=True)\n",
    ")\n",
    "\n",
    "# Prepare training settings\n",
    "epochs = 2\n",
    "lr_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Send model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Start training\n",
    "history = train_validate(\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    optim=optim,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
