{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ulckWoFVEfj"
   },
   "source": [
    "# Convolutional Neural Networks for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "610moVgqVAeW",
    "outputId": "9acf0521-9be9-45f0-8797-11c67aa30441"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 18:22:56.470744: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-20 18:22:56.470783: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-20 18:22:56.470830: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-20 18:22:56.480963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLOCk6P-fldm"
   },
   "source": [
    "## Task I: Word-based CNN for Text Classification\n",
    "\n",
    "### 1. Data\n",
    "\n",
    "The dataset that we are going to use is the imdb dataset of movie reviews. These are labelled by sentiment (positive/negative). \n",
    "\n",
    "The reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "\n",
    "For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "More information regarding the dataset can be found in the official [Keras documentation](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bR-rpN_HI0i_",
    "outputId": "40ff02f0-7829-48c5-bf7f-1e0d4b601c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWg-G68bh2uI"
   },
   "source": [
    "### 2. Preprocess the text data \n",
    "\n",
    "In this particular case, where we are using the imdb dataset there is no need to do all the traditional preprocessings that we normally do when dealing with NLP problems. Part of them are already done at this point.\n",
    "\n",
    "  - Split the dataset in train and test (maybe also validation).\n",
    "  - Tokenize and transform to integer index. Here we would need to: \n",
    "    - instantitate a *Tokenizer()* object, \n",
    "    - fit that object on the text on which we are training the model (use the *fit_on_texts()* method)\n",
    "    - call *texts_to_sequences()* for both the training and the test text.\n",
    "\n",
    "  - **Add padding to ensure that all vectors have the same dimensionality.** Note that this is the only pre-processing that needs to be done in the case of the imdb dataset that is ready to be imported from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDc5l6o2g6nM"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-b9f98ccc98a9>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-b9f98ccc98a9>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    x_train = # TODO\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "maxlen = 1000\n",
    "\n",
    "# TODO 2. Pad sequences\n",
    "\n",
    "x_train = # TODO\n",
    "x_test = # TODO\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtIeukSXltKp"
   },
   "source": [
    "### 3. CNN Architecture\n",
    "\n",
    "A very simple example of CNN architecture for text classification can be found [here](https://keras.io/examples/imdb_cnn/). Take a look in order to have an idea about how things should look like at this level.\n",
    "\n",
    "A few other simple implementations: https://www.kaggle.com/jacklinggu/keras-mlp-cnn-test-for-text-classification/notebook.\n",
    "\n",
    "Please note that if the dataset is too small and the complexity of the model is large, then the phenomenon of overfitting will be imminent and fast.\n",
    "\n",
    "The model will look similar to what you have seen in the CNN for image classification (the first lab):\n",
    "\n",
    "> **3.1.** Instantiate the model. Use the constructor of the Sequential class as you have done in the first laboratory.\n",
    "\n",
    "> **3.2.** After creating the initial model, we need to populate it with the hidden layers:\n",
    "- **3.2.1.** With text classification, the first layer is usually an **embedding layer** in which word indices that come from the input layer are converted to word vectors (word2vec). This is an important conversion which enables a more efficient and faster processing of textual data. Each word integer is mapped into a one dimensional vector of floats which captures its syntactical properties within the movie reviews text corpus. This subject may be covered in a little more depth in an upcoming laboratory. At the moment, a few insights have been introduced in the theoretical support for this lab and even more explanation can be found in the Tensorflow and Google code archives and other blogs:\n",
    "  - https://code.google.com/archive/p/word2vec\n",
    "  - https://www.tensorflow.org/tutorials/word2vec\n",
    "  - https://www.youtube.com/watch?v=T8tQZChniMk&list=PLR2RxXcwFe533vpJhgiDOAONyRzj3lzbJ\n",
    "  - http://colah.github.io/posts/2014-07-NLP-RNNs-Representations\n",
    "- **3.2.2.** After the embedding layer, add a **dropout** layer with a probability of 0.4.\n",
    "- **3.2.3.** Add a **CONV** layer with 128 filters, filter size 3, padding same, activation relu and stride 1. Since our word vectors are one dimensional, we only need 1-dim convolutions. Keras provides us with a built in method for doing it elegantly. Note that we need to specify a convolution kernel length and number of filters to use (nb_filter). More info about parameters and usage at: https://keras.io/layers/convolutional/\n",
    "- **3.2.4.** Use **Max Pooling** after the CONV layer.\n",
    "- **3.2.5.** Add another **CONV** layer, similar in terms of parameters with what we have at 3.2.3. The only difference is that here we are trying a different kernel size (5 to be more specific).\n",
    "- **3.2.6.** Add a **Batch Normalization** layer. You can, of course, try to train the model without this layer, or with any other modification. Without this layer in particular, you are going to (probably) see a fair amount of overfitting.\n",
    "- **3.2.7.** Use **Max Pooling** again.\n",
    "- **3.2.8.** Add a Flatten layer.\n",
    "- **3.2.9.** Add a dense layer with hidden_dims hidden units and activation relu.\n",
    "- **3.2.10. Dropout** neurons with a probability of 0.5. Do this in order to add even more prvention of overfitting.\n",
    "- **3.2.11.** Our output layer consists of one neuron. The sigmoid activation will produce a float number between 0 and 1. We can round it to 0 or 1 to conclude if the movie review is negative or positive, or we can interpret it as a fuzzy value of how much the review is positive or negative.\n",
    "\n",
    "> **3.3.** Compile the model using binary crossentropy as loss, accuracy as metric and adam optimizer. These values for the parameters should be seen as a suggestion and you are more than welcome to use any other valid values and see how the behavior changes.\n",
    "\n",
    "> **3.4.** Train (fit) the model on the training data, with the test set as validation data. Set shuffle to True. For this particular dataset,  one possible (decent) batch size is 64 and (maybe) 10 epochs should be enough in order to dive into the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5D4lUtmmA37"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_features = 10000\n",
    "batch_size = 64\n",
    "embedding_dims = 100\n",
    "filters = 128\n",
    "ks = [3, 5, 5] # kernel_size\n",
    "hidden_dims = 128\n",
    "epochs = 10\n",
    "\n",
    "def get_cnn_model_v1(): \n",
    "    # TODO 3.1. Create the model, no layers yet\n",
    "    model = # TODO\n",
    "\n",
    "    # 3.2. Add the layers (check out the work done in the previous lab)\n",
    "    \n",
    "    ########################################################################################\n",
    "    # 3.2.1. Add an embedding layer which maps our vocab indices (max_features) into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "    \n",
    "    # TODO 3.2.2. Dropout with a probability of 0.4\n",
    "\n",
    "    # TODO 3.2.3. Add a Convolution1D layer, with 128 filters, kernel size ks[0], padding same, activation relu and stride 1\n",
    "    \n",
    "    # TODO 3.2.4. Use max pooling after the CONV layer\n",
    "    \n",
    "    # TODO 3.2.5. Add a CONV layer, similar in properties to what we have above (3.2.3.) and kernel size 5\n",
    "    \n",
    "    # TODO 3.2.6. Add a Batch Normalization layer in order to reduce overfitting\n",
    "    \n",
    "    # TODO 3.2.7. Use max pooling again\n",
    "\n",
    "    # TODO 3.2.8. Add a flatten layer\n",
    "\n",
    "    # TODO 3.2.9. Add a dense layer with hidden_dims hidden units and activation relu\n",
    "    \n",
    "    # TODO 3.2.10. Add a dropout layer with a dropout probability of 0.5 \n",
    "\n",
    "    # TODO 3.2.11. We project onto a single unit output layer, and squash it with a sigmoid\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    ##################################################################################\n",
    "\n",
    "    # TODO 3.3. Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "  \n",
    "model = get_cnn_model_v1()\n",
    "\n",
    "# TODO 3.4. Train (fit) the model\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMiGHkMitRLw"
   },
   "source": [
    "### 4. Evaluate the model\n",
    "\n",
    "Evaluate the loss and the metrics (in our case accuracy) by which the model is evaluated for both:\n",
    "\n",
    "> **4.1.** **Training** - This should show a number around the value 0.96.\n",
    "\n",
    "> **4.2.** **Test** - The accuracy on the test set should show a value around 0.87.\n",
    "\n",
    "Even from the values above, can be seen the overfitting of the model.\n",
    "And, again, the model (hyperparameters) and choices of functions, etc are only to show how you can implement a CNN for text classification. From here you can try all kinds of new configurations, network architectures and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "dB6M_G89tUpB",
    "outputId": "aaac009b-43a4-4130-af58-05d3efa0622c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 7s 278us/step\n",
      "Training Accuracy: 0.9809\n",
      "Testing Accuracy:  0.8780\n"
     ]
    }
   ],
   "source": [
    "# 4.1. Evaluate the accuracy and loss on the training set\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "# 4.2. Evaluate the accuracy and loss on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64OoW_8Cvw4A"
   },
   "source": [
    "### 5. Visualize accuracy\n",
    "\n",
    "The following plots show the learning curves during training/test in terms of accuracy and loss.\n",
    "As a bonus, time permitting or as a homework, try to improve these by changing the architecture of the CNN and the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQRz2whBv0JZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUuYjnVLYCHf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8X-vRKpwNOL"
   },
   "source": [
    "### 6. Save/Load model\n",
    "\n",
    "> 6.1. Exercise your knowledge on how to **save**\n",
    "\n",
    "> 6.2. and **load** a model using the capabilities offered by the Keras framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQz_R6xPwQR2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# TODO 6.1. Save the model\n",
    "\n",
    "# TODO 6.2. Load the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ityrg0zWAqSE"
   },
   "source": [
    "## Task II: Character-based CNN for Text Classification\n",
    "\n",
    "Time permitting, implement the character-based version of the CNN which is able to classify the movie reviews by sentiment.\n",
    "\n",
    "You can have a look at the few implementations that can be found online. For example, this implementation is really similar with what has been implemented in the first paper where the problem of character-based CNN is formally tackled: https://github.com/chaitjo/character-level-cnn/blob/master/models/char_cnn_kim.py. \n",
    "\n",
    "However, keep in mind that for this particular dataset there are some modifications that need to be done. Your challenge is to discover those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gc1Ru4E4unDp"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0kdmHKPrqOA"
   },
   "source": [
    "### 1. Restore the strings in the dataset\n",
    "In this class, we have decided to use the imdb dataset from keras datasets. The only problem with this when trying to implement a character-based NN is that it has already performed the word2vec changes. This optimization, as we have seen in the Task I, is a really helpful one when implementing a word-based CNN. However this is not the case for a character based CNN, as we need to extract some information from the text, in order to be able to build the model.\n",
    "\n",
    "> **1.1.** **Map IDs to words**\n",
    "- **1.1.1.** Get the indexes corresponding to each word as per the imdb dataset (and module in keras). Hint: look up get_word_index().\n",
    "- **1.1.2.** Shift with 3 the values in the word_to_id dictionary populated above. In other words, for key k, the value is going to be v + 3 instead of v now. We do this because the first 3 indexes are reserved for special symbols (see at the next step).\n",
    "- **1.1.3.** Indexes 0-3 are now taken by the following special characters: \"\\<PAD>\", \"\\<START>\", \"\\<UNK>\", \"\\<UNUSED>\". Mark this by assigning the appropriate values to those keys.\n",
    "- **1.1.4.** Reverse the roles in word_to_id. Build an id_to_word dictionary with the same content, and the difference that here the keys are the values from word_to_id and the values are the keys from that dictionary. We are doing this in order to ease the (re)conversion to text. \n",
    "\n",
    "> **1.2.** **Restore the text data**\n",
    "- **1.2.1.** Get the corresponding words for the ids in the dataset. Use the function implemented following the steps above.\n",
    "- **1.2.2.** For each sample (list of indexes corresponding to words in that review) in the dataset, perform the conversion to string, appending the words found at those indexes. Hint: use join() maybe and list comprehension.\n",
    "\n",
    "> **1.3.** **Load the dataset** with only the top 5000 words.\n",
    "\n",
    "> **1.4.** Restore both the training and test text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HywaOzWPAvg3"
   },
   "outputs": [],
   "source": [
    "def map_ids_to_words():\n",
    "    \"\"\"\n",
    "    1.1. Map ids to words\n",
    "    \n",
    "    This method returns a dictionary representing a mapping id -> word like \n",
    "    it is in the imdb dataset from keras datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO 1.1.1. Get word indexes from the imdb dataset\n",
    "    word_to_id = # TODO\n",
    "    \n",
    "    # 1.1.2. Shift with 3 (value + 3) the indexes in word_to_id\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    \n",
    "    # TODO 1.1.3. Indexes 0-3 are now taken by the following special characters: \"<PAD>\", \"<START>\", \"<UNK>\", \"<UNUSED>\".\n",
    "\n",
    "    # 1.1.4. Reverse the roles in word_to_id (now the values become keys)\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "    \n",
    "    return id_to_word\n",
    "\n",
    "def restore_text_data(dataset):\n",
    "    \"\"\"\n",
    "    1.2. Restore the text data.\n",
    "    \n",
    "    Use the method declared above to mapt the ids in the imdb representation to actual words.\n",
    "    \"\"\"\n",
    "    str_data = []\n",
    "    \n",
    "    # TODO 1.2.1. Get the corresponding words for the ids in the dataset.\n",
    "    id_to_word = #TODO\n",
    "    \n",
    "    # 1.2.2. For each sample in the dataset, perform the conversion to string\n",
    "    for i in range(len(dataset)):\n",
    "        str_data.append(' '.join(id_to_word[id] for id in dataset[i] ))\n",
    "  \n",
    "    print(str_data[0])\n",
    "    \n",
    "    return str_data\n",
    "\n",
    "# 1.3. Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000, index_from=3)\n",
    "\n",
    "# 1.4. Restore text data for both training and test\n",
    "X_train_str = restore_text_data(x_train)\n",
    "X_test_str = restore_text_data(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wb0dWiRm4IhX"
   },
   "source": [
    "### 2. Get the vocabulary\n",
    "> **2.1.** Get the vocabulary - identify the set of characters (\"vocabulary\") for this dataset.\n",
    "\n",
    "### 3. Create a Tokenizer\n",
    "\n",
    "> **3.1.** Initialize the tokenizer. The important parameters here are:\n",
    "- **char_level**=True: this can tell tk.texts_to_sequences() to process sentence in char level.\n",
    "- **oov_token**='UNK': this will add a UNK token in the vocabulary. We can call it by tk.oov_token.\n",
    "- **num_words**=None for starters.\n",
    "\n",
    "> **3.2.** After calling tk.fit_on_texts(X_train_str), the tk object will contain the neccery information about the training data. \n",
    "\n",
    "> **3.3.** Build a character dictionary based on our alphabet (chars). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYq43OAXuxw-"
   },
   "outputs": [],
   "source": [
    "# Maximum length for a sequence\n",
    "maxlen = 1024 \n",
    "\n",
    "def get_vocabulary(X_train_str):\n",
    "    \"\"\"\n",
    "    2. Get the vocabulary.\n",
    "    \n",
    "    For each document in the dataset, for each symbol in that document, \n",
    "    append that symbol to txt.\n",
    "    \"\"\"\n",
    "    txt = ''\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return set(txt)\n",
    "  \n",
    "def create_tokenizer(X_train_str, chars):\n",
    "    \"\"\"\n",
    "    3. Create a Tokenizer.\n",
    "    \"\"\"\n",
    "    # TODO 3.1. Initialize the Tokenizer\n",
    "    tk = # TODO\n",
    "\n",
    "    # TODO 3.2. Fit the tokenizer on the training data\n",
    "\n",
    "    # 3.3. Build a character dictionary based on our alphabet (chars)\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(chars):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "    # TODO 3.4. Use char_dict to replace the tk.word_index\n",
    "    tk.word_index = # TODO\n",
    "\n",
    "    # 3.5. Add 'UNK' to the vocabulary \n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "    \n",
    "    return tk\n",
    "    \n",
    "chars = get_vocabulary(X_train_str)\n",
    "\n",
    "tk = create_tokenizer(X_train_str, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgFs4fobFdP3"
   },
   "source": [
    "### 4. Yet another few preprocessings before building the actual model\n",
    "\n",
    "> **4.1.** Conversions to be done for both the test/training text.\n",
    "- **4.1.1.** Convert the strings in the text datasets to arrays of indexes. Hint: use texts_to_sequences().\n",
    "- **4.1.2.** The sequences need to have the same length, so pad_sequences() until the maxlen of 1024.\n",
    "- **4.1.3.** Convert to numpy array.\n",
    "\n",
    "> **4.2.** One-hot encode the set of targets for the training.\n",
    "\n",
    "> **4.3.** One-hot encode the set of targets for the test. Hint: to_categorical()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgU6zBAb4wVp"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tk):\n",
    "    \"\"\"\n",
    "    4.1. Conversions to be done for both the test/training text.\n",
    "    \"\"\"\n",
    "    # TODO 4.1.1. Convert string to index \n",
    "    sequences = # TODO\n",
    "\n",
    "    # TODO 4.1.2. Padding\n",
    "    proc_data = # TODO\n",
    "\n",
    "    # TODO 4.1.3. Convert to numpy array\n",
    "    proc_data = # TODO\n",
    "    \n",
    "    return proc_data\n",
    "\n",
    "train_data = preprocess_dataset(X_train_str, tk)\n",
    "test_data = preprocess_dataset(X_test_str, tk)\n",
    "\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "print(\"vocabulary: \", tk.word_index)\n",
    "print(\"vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQqJGVfOu_iL"
   },
   "outputs": [],
   "source": [
    "# TODO 4.2. One-hot encode the training targets\n",
    "y_train = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EvdlVGEvFIP"
   },
   "outputs": [],
   "source": [
    "# TODO 4.3. One-hot encode the test targets\n",
    "y_test = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Mm3Pxu9KCRU"
   },
   "source": [
    "### 5. Load the Embedding Weights\n",
    "In order to understand how to assign embedding weights to the embedding layer, here we initialize the embedding weights manually instead of initializing it randomly.\n",
    "\n",
    "> **5.1.** **Append a zero vector** of size vocab_size, to represent the PAD. We can see, besides the 49 characters, we also have a UNK(unknown token) to represent the rare characters in vocabulary. Then we use the one-hot vector to represent these 50 characters, which means each character has 50 dimensions. Because Keras use 0 for PAD, we add a zero vector to represent PAD.\n",
    "\n",
    "> **5.2.** One-hot encoding used for the embedding of the weights.\n",
    "\n",
    "> **5.3.** Convert the embedding weights to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3vZwhbb6aWx"
   },
   "outputs": [],
   "source": [
    "def load_embedding_weights(tk, vocab_size):\n",
    "    \"\"\"\n",
    "    5. Load manually the embedding weights.\n",
    "    \"\"\"\n",
    "    embedding_weights = [] #(51, 50)\n",
    "\n",
    "    # TODO 5.1. Append zero vector to represent the PAD\n",
    "\n",
    "    # 5.2. One-hot repres of the characters\n",
    "    for char, i in tk.word_index.items():\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "\n",
    "    # TODO 5.3. Convert to a numpy array\n",
    "    embedding_weights = # TODO\n",
    "    \n",
    "    return embedding_weights\n",
    "  \n",
    "embedding_weights = load_embedding_weights(tk, vocab_size)\n",
    "\n",
    "print(\"embedding weights shape: \", embedding_weights.shape)\n",
    "print(\"embedding weights: \\n\", embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISNn5UbqSJEJ"
   },
   "source": [
    "### 6. Build the model\n",
    "\n",
    "> **6.1.** Instatiate the sequential model with no layers.\n",
    "\n",
    "> **6.2.** Add an Embedding layer with vocab_size + 1, embedding_size, input_size as input_length and embedding_weights as weights.\n",
    "\n",
    "> **6.3.** Add the convolutional layers having the \\[filter_num, filter_size, pooling_size] specified in conv_layers. Note that if the pooling_size is -1, there is no pooling layer after that CONV. The activation function used is relu.\n",
    "\n",
    "> **6.4.** Add a Flatten layer.\n",
    "\n",
    "> **6.5.** Add one Fully connected layer with relu as activation and 1024 hidden units.\n",
    "\n",
    "> **6.6.** Dropout half of the neurons.\n",
    "\n",
    "> **6.7.** The output layer uses sigmoid as activation function and has 2 nodes.\n",
    "\n",
    "> **6.8.** Compile the model with loss, optimizer and accuracy as metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWAvLuXi7rUu"
   },
   "outputs": [],
   "source": [
    "input_size = 1024\n",
    "embedding_size = 50\n",
    "num_of_classes = 2\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = \"binary_crossentropy\"\n",
    "\n",
    "conv_layers = [[128, 7, 3], [128, 5, -1], [128, 3, -1], [128, 3, 3]]\n",
    "\n",
    "def build_model_2():\n",
    "    # TODO 6.1. Instantiate the model\n",
    "    model = # TODO\n",
    "\n",
    "    # 6.2. Add an Embedding layer \n",
    "    model.add(Embedding(vocab_size+1, embedding_size, input_length=input_size, weights=[embedding_weights]))\n",
    "    \n",
    "    # TODO 6.3. Add the convolutional layers having the [filter_num, filter_size, pooling_size] specified in conv_layers\n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        # TODO\n",
    "        \n",
    "    # TODO 6.4. Add a Flatten layer\n",
    "\n",
    "    # TODO 6.5. Add one Fully connected layer\n",
    "    \n",
    "    # TODO 6.6. Dropout\n",
    "\n",
    "    # TODO 6.7. Output layer\n",
    "\n",
    "    # TODO 6.8. Compile the model\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "  \n",
    "model2 = build_model_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ad7YlXucTiy3"
   },
   "source": [
    "### 7. Train the model\n",
    "\n",
    "We have experimented with batch size 128, 10 epochs and the train and validation (test_data) as parameters. Verbose should be 1 in order to follow how the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLT8d4LPFjth"
   },
   "outputs": [],
   "source": [
    "# TODO 7. Training\n",
    "history2 = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOHiRwzDUBQ5"
   },
   "source": [
    "### 8. Evaluate the model\n",
    "\n",
    "> **8.1.** Evaluate the accuracy and loss on the training set.\n",
    "\n",
    "> **8.2.** Evaluate the accuracy and loss on the test set.\n",
    "\n",
    "> **8.3.** Plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QELLHdWcf3hE"
   },
   "outputs": [],
   "source": [
    "# TODO 8.1. Evaluate the accuracy and loss on the training set\n",
    "loss, accuracy = # TODO\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "# TODO 8.2. Evaluate the accuracy and loss on the test set\n",
    "loss, accuracy = # TODO\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h04vxn18f-aU"
   },
   "outputs": [],
   "source": [
    "# 8.3. Plot the learning curves\n",
    "%matplotlib inline\n",
    "plot_history(history2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Lab2_CNN_for_Text_Classification_SKEL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
